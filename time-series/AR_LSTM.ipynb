{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a0118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08fc11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4550, 37)\n",
      "     state    month     vmt      ndvi  flights   co_mean  co_max1_value  \\\n",
      "0  Alabama  2018-01  5101.0  0.552518   6819.0  0.270312       0.527258   \n",
      "1  Alabama  2018-02  4975.0  0.541536   6417.0  0.219343       0.402143   \n",
      "2  Alabama  2018-03  5952.0  0.565882   7507.0  0.226716       0.407704   \n",
      "3  Alabama  2018-04  6145.0  0.679999   7541.0  0.214604       0.392311   \n",
      "4  Alabama  2018-05  6253.0  0.761965   8002.0  0.302609       0.490366   \n",
      "\n",
      "   co_max1_hour    co_aqi   no2_mean  ...  o3_max1_hour     o3_aqi      awnd  \\\n",
      "0     10.153226  5.209677  12.320599  ...     11.854839  30.725806  3.119355   \n",
      "1     10.383929  3.946429   8.593592  ...     21.219246  22.709325  3.514286   \n",
      "2      8.952151  3.747312   9.844573  ...     10.468993  40.165997  3.558065   \n",
      "3      8.936111  3.705556   8.937356  ...     10.388948  45.017963  3.526667   \n",
      "4      9.212366  4.946237  11.115237  ...      9.495067  45.172110  2.141935   \n",
      "\n",
      "    prcp       tavg       tmax       tmin  year  month_num  observed  \n",
      "0   34.4   4.341935  10.332258  -1.167742  2018          1      True  \n",
      "1  206.8  14.110714  19.650000   9.039286  2018          2      True  \n",
      "2  104.9  12.977419  19.358065   6.551613  2018          3      True  \n",
      "3  209.6  15.720000  22.576667   8.696667  2018          4      True  \n",
      "4  177.2  23.754839  29.883871  18.393548  2018          5      True  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"MasterDataset.csv\")\n",
    "\n",
    "df = df.drop(columns=['snow','snwd']) # drop snow and snwd\n",
    "\n",
    "df[\"month\"] = pd.to_datetime(df[\"month\"], format=\"%Y-%m\").dt.to_period(\"M\")\n",
    "\n",
    "df[\"year\"] = df[\"month\"].dt.year\n",
    "df[\"month_num\"] = df[\"month\"].dt.month\n",
    "\n",
    "# fill dates from Jan 2018 and Jul 2025\n",
    "states = sorted(df[\"state\"].unique())\n",
    "all_months = pd.period_range(\"2018-01\", \"2025-07\", freq=\"M\") \n",
    "\n",
    "base_panel = (\n",
    "    pd.MultiIndex.from_product([states, all_months], names=[\"state\", \"month\"])\n",
    "    .to_frame(index=False)\n",
    ")\n",
    "\n",
    "# mering\n",
    "full = base_panel.merge(\n",
    "    df,\n",
    "    on=[\"state\", \"month\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# 6. Mark which rows were originally present\n",
    "full[\"observed\"] = full[\"_merge\"].eq(\"both\")\n",
    "full.drop(columns=\"_merge\", inplace=True)\n",
    "\n",
    "\n",
    "#sort\n",
    "full = full.sort_values([\"state\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "full.to_csv(\"MasterDataset_2018_2025_fullpanel.csv\", index=False)\n",
    "print(full.shape)\n",
    "print(full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e0554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820\n"
     ]
    }
   ],
   "source": [
    "# # Count how many months are actually populated per state-year in the full panel\n",
    "# month_counts = (\n",
    "#     full.groupby([\"state\", \"year\"])[\"month\"]\n",
    "#         .nunique()\n",
    "#         .reset_index(name=\"n_months\")\n",
    "# )\n",
    "full = full.sort_values([\"state\", \"month\"])\n",
    "\n",
    "numeric_cols = full.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "for drop_col in [\"year\", \"month_num\", \"month\"]:\n",
    "    if drop_col in numeric_cols:\n",
    "        numeric_cols.remove(drop_col)\n",
    "\n",
    "# polynomial interpolation per state w degree 2\n",
    "full[numeric_cols] = full.groupby(\"state\")[numeric_cols].transform(\n",
    "    lambda g: g.interpolate(method=\"polynomial\", order=2)\n",
    ")\n",
    "\n",
    "full[numeric_cols] = full.groupby(\"state\")[numeric_cols].transform(\n",
    "    lambda g: g.ffill().bfill()\n",
    ")\n",
    "\n",
    "print(full[numeric_cols].isnull().sum().sum())\n",
    "\n",
    "full.to_csv(\"MasterDataset_interpolated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5859330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining NAs: 0\n",
      "no2_mean           0\n",
      "no2_max1_value     0\n",
      "no2_max1_hour      0\n",
      "no2_aqi            0\n",
      "pm10_mean          0\n",
      "pm10_max1_value    0\n",
      "pm10_max1_hour     0\n",
      "pm10_aqi           0\n",
      "tavg               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## to fill the 1820 (they're fully missing so interpolation won't work\n",
    "#print(full.isna().sum())\n",
    "\n",
    "# Groups of columns that still have NA\n",
    "cols_no2 = [\"no2_mean\", \"no2_max1_value\", \"no2_max1_hour\", \"no2_aqi\"]\n",
    "cols_pm10 = [\"pm10_mean\", \"pm10_max1_value\", \"pm10_max1_hour\", \"pm10_aqi\"]\n",
    "cols_tavg = [\"tavg\"]\n",
    "\n",
    "# For each month, fill remaining NAs with monthly mean\n",
    "for cols in [cols_no2, cols_pm10, cols_tavg]:\n",
    "    for col in cols:\n",
    "        full[col] = full.groupby(\"month\")[col].transform(\n",
    "            lambda s: s.fillna(s.mean())\n",
    "        )\n",
    "\n",
    "print(\"Total remaining NAs:\", full.isna().sum().sum())\n",
    "print(full[cols_no2 + cols_pm10 + cols_tavg].isna().sum())\n",
    "\n",
    "#\n",
    "full.to_csv(\"MasterDataset_interpolated_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "667eaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(feature_cols): 32\n",
      "['vmt', 'ndvi', 'flights', 'co_mean', 'co_max1_value', 'co_max1_hour', 'co_aqi', 'no2_mean', 'no2_max1_value', 'no2_max1_hour', 'no2_aqi', 'pm25_max1_value', 'pm25_max1_hour', 'pm25_aqi', 'pm10_mean', 'pm10_max1_value', 'pm10_max1_hour', 'pm10_aqi', 'so2_mean', 'so2_max1_value', 'so2_max1_hour', 'so2_aqi', 'o3_mean', 'o3_max1_value', 'o3_max1_hour', 'o3_aqi', 'awnd', 'prcp', 'tavg', 'tmax', 'tmin', 'month_num']\n",
      "Train: 2018-01 -> 2021-12\n",
      "Val:   2022-01 -> 2023-12\n",
      "Test:  2024-01 -> 2025-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malarmuthu/conda/envs/dva_env/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/malarmuthu/conda/envs/dva_env/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/malarmuthu/conda/envs/dva_env/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = full.copy()\n",
    "\n",
    "df[\"month\"] = df[\"month\"].astype(\"period[M]\")\n",
    "\n",
    "# Sort\n",
    "df = df.sort_values([\"state\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "# Target\n",
    "target_col = \"pm25_mean\"   \n",
    "\n",
    "continuous_cols = [\n",
    "    'vmt','ndvi','flights',\n",
    "    'co_mean','co_max1_value','co_max1_hour','co_aqi',\n",
    "    'no2_mean','no2_max1_value','no2_max1_hour','no2_aqi',\n",
    "    'pm25_max1_value','pm25_max1_hour','pm25_aqi',\n",
    "    'pm10_mean','pm10_max1_value','pm10_max1_hour','pm10_aqi',\n",
    "    'so2_mean','so2_max1_value','so2_max1_hour','so2_aqi',\n",
    "    'o3_mean','o3_max1_value','o3_max1_hour','o3_aqi',\n",
    "    'awnd','prcp','tavg','tmax','tmin'\n",
    "] \n",
    "feature_cols = continuous_cols + ['month_num']\n",
    "\n",
    "print(\"len(feature_cols):\", len(feature_cols))\n",
    "print(feature_cols)\n",
    "\n",
    "train_end = pd.Period(\"2021-12\", freq=\"M\")\n",
    "val_end   = pd.Period(\"2023-12\", freq=\"M\")\n",
    "\n",
    "train_df = df[df[\"month\"] <= train_end]\n",
    "val_df   = df[(df[\"month\"] > train_end) & (df[\"month\"] <= val_end)]\n",
    "test_df  = df[df[\"month\"] > val_end]\n",
    "\n",
    "print(\"Train:\", train_df[\"month\"].min(), \"->\", train_df[\"month\"].max())\n",
    "print(\"Val:  \", val_df[\"month\"].min(),   \"->\", val_df[\"month\"].max())\n",
    "print(\"Test: \", test_df[\"month\"].min(),  \"->\", test_df[\"month\"].max())\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler().fit(train_df[feature_cols])\n",
    "\n",
    "def apply_scaler(local_df):\n",
    "    d = local_df.copy()\n",
    "    d[feature_cols] = scaler.transform(d[feature_cols].values)\n",
    "    return d\n",
    "\n",
    "train_df_s = apply_scaler(train_df)\n",
    "val_df_s   = apply_scaler(val_df)\n",
    "test_df_s  = apply_scaler(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f053a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X: (1800, 12, 32)\n",
      "train_y: (1800,)\n",
      "val_X: (600, 12, 32)\n",
      "val_y: (600,)\n",
      "test_X: (350, 12, 32)\n",
      "test_y: (350,)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 12\n",
    "def build_sequences(df, seq_len, feature_cols, target_col):\n",
    "    sequences_X, sequences_y = [], []\n",
    "    \n",
    "    for state in df['state'].unique():\n",
    "        state_data = df[df['state'] == state].sort_values('month')\n",
    "        state_features = state_data[feature_cols].values\n",
    "        state_target = state_data[target_col].values\n",
    "        \n",
    "        for i in range(len(state_data) - seq_len):\n",
    "            sequences_X.append(state_features[i:(i + seq_len)])\n",
    "            sequences_y.append(state_target[i + seq_len])\n",
    "    \n",
    "    return np.array(sequences_X), np.array(sequences_y)\n",
    "\n",
    "train_X, train_y = build_sequences(train_df_s, seq_len, feature_cols, target_col)\n",
    "val_X,   val_y   = build_sequences(val_df_s,   seq_len, feature_cols, target_col)\n",
    "test_X,  test_y  = build_sequences(test_df_s,  seq_len, feature_cols, target_col)\n",
    "\n",
    "print(\"train_X:\", train_X.shape)\n",
    "print(\"train_y:\", train_y.shape)\n",
    "print(\"val_X:\",   val_X.shape)\n",
    "print(\"val_y:\",   val_y.shape)\n",
    "print(\"test_X:\",  test_X.shape)\n",
    "print(\"test_y:\",  test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e535b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 12, 64)            24832     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39905 (155.88 KB)\n",
      "Trainable params: 39905 (155.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "n_timesteps = train_X.shape[1]\n",
    "n_features  = train_X.shape[2]\n",
    "\n",
    "l2_reg = regularizers.l2(1e-4) #regularisation\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(n_timesteps, n_features)),\n",
    "    layers.LSTM(64, return_sequences=True,kernel_regularizer=l2_reg, recurrent_regularizer=l2_reg),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32,return_sequences=False,kernel_regularizer=l2_reg,recurrent_regularizer=l2_reg),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation=\"relu\",kernel_regularizer=l2_reg),\n",
    "    layers.Dense(32, activation=\"relu\",kernel_regularizer=l2_reg),\n",
    "    layers.Dense(16,activation=\"relu\"),\n",
    "    layers.Dense(1) \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"mse\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab0245c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 34.4476 - mae: 4.5018 - val_loss: 13.5134 - val_mae: 2.2409 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 12.0494 - mae: 1.9070 - val_loss: 11.7761 - val_mae: 2.0191 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.6468 - mae: 1.7497 - val_loss: 10.9712 - val_mae: 1.9720 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.5883 - mae: 1.7346 - val_loss: 10.9762 - val_mae: 1.9684 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.0640 - mae: 1.6611 - val_loss: 10.1733 - val_mae: 1.9299 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.8168 - mae: 1.6615 - val_loss: 11.9310 - val_mae: 2.0751 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.6976 - mae: 1.6745 - val_loss: 10.0263 - val_mae: 1.8253 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.8240 - mae: 1.5781 - val_loss: 10.9222 - val_mae: 1.9386 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.8214 - mae: 1.5259 - val_loss: 9.8231 - val_mae: 1.8143 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.3803 - mae: 1.5396 - val_loss: 9.9285 - val_mae: 1.9162 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8426 - mae: 1.5228 - val_loss: 9.0711 - val_mae: 1.7602 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6357 - mae: 1.4856 - val_loss: 11.4558 - val_mae: 2.0374 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.2944 - mae: 1.4505 - val_loss: 9.9671 - val_mae: 1.8245 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.7371 - mae: 1.3971 - val_loss: 11.1889 - val_mae: 1.9936 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.1826 - mae: 1.3618 - val_loss: 11.7853 - val_mae: 2.0483 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.6791 - mae: 1.3053 - val_loss: 12.4917 - val_mae: 2.1390 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_X, train_y,\n",
    "    validation_data=(val_X, val_y),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop,reduce_lr],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f371f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training LSTM with target: vmt\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target vmt:\n",
      "    no2_max1_value: ΔMSE = 0.0081\n",
      "    o3_max1_value: ΔMSE = 0.0066\n",
      "    o3_max1_hour: ΔMSE = 0.0038\n",
      "    pm25_aqi: ΔMSE = 0.0037\n",
      "    pm25_max1_value: ΔMSE = 0.0033\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: ndvi\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target ndvi:\n",
      "    month_num: ΔMSE = 0.0909\n",
      "    tavg: ΔMSE = 0.0216\n",
      "    pm25_aqi: ΔMSE = 0.0188\n",
      "    tmin: ΔMSE = 0.0163\n",
      "    tmax: ΔMSE = 0.0137\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: flights\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target flights:\n",
      "    vmt: ΔMSE = 0.1540\n",
      "    o3_aqi: ΔMSE = 0.0128\n",
      "    awnd: ΔMSE = 0.0110\n",
      "    pm10_max1_hour: ΔMSE = 0.0085\n",
      "    pm10_mean: ΔMSE = 0.0073\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: co_mean\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target co_mean:\n",
      "    co_aqi: ΔMSE = 0.0659\n",
      "    ndvi: ΔMSE = 0.0646\n",
      "    co_max1_value: ΔMSE = 0.0553\n",
      "    flights: ΔMSE = 0.0443\n",
      "    so2_max1_hour: ΔMSE = 0.0326\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: co_max1_value\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target co_max1_value:\n",
      "    co_aqi: ΔMSE = 0.1054\n",
      "    month_num: ΔMSE = 0.0431\n",
      "    ndvi: ΔMSE = 0.0218\n",
      "    co_mean: ΔMSE = 0.0216\n",
      "    pm10_max1_value: ΔMSE = 0.0190\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: co_max1_hour\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target co_max1_hour:\n",
      "    month_num: ΔMSE = 0.0462\n",
      "    no2_mean: ΔMSE = 0.0371\n",
      "    ndvi: ΔMSE = 0.0241\n",
      "    so2_mean: ΔMSE = 0.0204\n",
      "    vmt: ΔMSE = 0.0194\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: co_aqi\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target co_aqi:\n",
      "    co_mean: ΔMSE = 0.0492\n",
      "    co_max1_value: ΔMSE = 0.0360\n",
      "    month_num: ΔMSE = 0.0204\n",
      "    ndvi: ΔMSE = 0.0203\n",
      "    awnd: ΔMSE = 0.0142\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: no2_mean\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target no2_mean:\n",
      "    month_num: ΔMSE = 0.0391\n",
      "    no2_aqi: ΔMSE = 0.0326\n",
      "    no2_max1_value: ΔMSE = 0.0249\n",
      "    tavg: ΔMSE = 0.0147\n",
      "    pm25_max1_hour: ΔMSE = 0.0114\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: no2_max1_value\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target no2_max1_value:\n",
      "    month_num: ΔMSE = 0.0727\n",
      "    no2_aqi: ΔMSE = 0.0637\n",
      "    no2_mean: ΔMSE = 0.0610\n",
      "    pm25_max1_hour: ΔMSE = 0.0114\n",
      "    tmax: ΔMSE = 0.0095\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: no2_max1_hour\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target no2_max1_hour:\n",
      "    ndvi: ΔMSE = 0.0176\n",
      "    month_num: ΔMSE = 0.0163\n",
      "    flights: ΔMSE = 0.0104\n",
      "    tavg: ΔMSE = 0.0102\n",
      "    o3_aqi: ΔMSE = 0.0102\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: no2_aqi\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target no2_aqi:\n",
      "    no2_max1_value: ΔMSE = 0.0798\n",
      "    month_num: ΔMSE = 0.0593\n",
      "    no2_mean: ΔMSE = 0.0247\n",
      "    o3_aqi: ΔMSE = 0.0176\n",
      "    pm10_aqi: ΔMSE = 0.0146\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: pm25_max1_value\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target pm25_max1_value:\n",
      "    o3_mean: ΔMSE = 0.0298\n",
      "    pm25_max1_hour: ΔMSE = 0.0172\n",
      "    co_max1_hour: ΔMSE = 0.0137\n",
      "    o3_max1_value: ΔMSE = 0.0084\n",
      "    pm25_aqi: ΔMSE = 0.0051\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: pm25_max1_hour\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target pm25_max1_hour:\n",
      "    awnd: ΔMSE = 0.0098\n",
      "    so2_max1_value: ΔMSE = 0.0064\n",
      "    so2_mean: ΔMSE = 0.0060\n",
      "    prcp: ΔMSE = 0.0057\n",
      "    no2_max1_value: ΔMSE = 0.0050\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: pm25_aqi\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target pm25_aqi:\n",
      "    flights: ΔMSE = 0.0835\n",
      "    o3_aqi: ΔMSE = 0.0613\n",
      "    ndvi: ΔMSE = 0.0424\n",
      "    o3_max1_value: ΔMSE = 0.0324\n",
      "    vmt: ΔMSE = 0.0316\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: pm10_mean\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target pm10_mean:\n",
      "    o3_mean: ΔMSE = 0.0540\n",
      "    o3_max1_value: ΔMSE = 0.0245\n",
      "    month_num: ΔMSE = 0.0210\n",
      "    vmt: ΔMSE = 0.0150\n",
      "    o3_aqi: ΔMSE = 0.0128\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: pm10_max1_value\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target pm10_max1_value:\n",
      "    month_num: ΔMSE = 0.0471\n",
      "    o3_mean: ΔMSE = 0.0457\n",
      "    o3_aqi: ΔMSE = 0.0269\n",
      "    no2_aqi: ΔMSE = 0.0106\n",
      "    o3_max1_value: ΔMSE = 0.0102\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: pm10_max1_hour\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target pm10_max1_hour:\n",
      "    ndvi: ΔMSE = 0.0065\n",
      "    pm25_aqi: ΔMSE = 0.0062\n",
      "    pm25_max1_value: ΔMSE = 0.0053\n",
      "    no2_max1_value: ΔMSE = 0.0038\n",
      "    o3_max1_value: ΔMSE = 0.0030\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: pm10_aqi\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target pm10_aqi:\n",
      "    month_num: ΔMSE = 0.0602\n",
      "    o3_aqi: ΔMSE = 0.0179\n",
      "    flights: ΔMSE = 0.0155\n",
      "    o3_mean: ΔMSE = 0.0154\n",
      "    o3_max1_value: ΔMSE = 0.0141\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: so2_mean\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target so2_mean:\n",
      "    pm10_aqi: ΔMSE = 0.0363\n",
      "    pm10_mean: ΔMSE = 0.0301\n",
      "    ndvi: ΔMSE = 0.0210\n",
      "    o3_mean: ΔMSE = 0.0200\n",
      "    o3_max1_hour: ΔMSE = 0.0141\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: so2_max1_value\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target so2_max1_value:\n",
      "    so2_mean: ΔMSE = 0.7076\n",
      "    so2_aqi: ΔMSE = 0.0878\n",
      "    o3_aqi: ΔMSE = 0.0383\n",
      "    pm10_max1_hour: ΔMSE = 0.0175\n",
      "    month_num: ΔMSE = 0.0150\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: so2_max1_hour\n",
      "======================================================================\n",
      "  Top 5 features for target so2_max1_hour:\n",
      "    pm25_max1_hour: ΔMSE = 0.0147\n",
      "    pm10_max1_hour: ΔMSE = 0.0129\n",
      "    o3_mean: ΔMSE = 0.0113\n",
      "    o3_max1_value: ΔMSE = 0.0094\n",
      "    awnd: ΔMSE = 0.0088\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: so2_aqi\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target so2_aqi:\n",
      "    so2_mean: ΔMSE = 0.0234\n",
      "    pm10_aqi: ΔMSE = 0.0190\n",
      "    flights: ΔMSE = 0.0143\n",
      "    co_aqi: ΔMSE = 0.0116\n",
      "    o3_max1_value: ΔMSE = 0.0108\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: o3_mean\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target o3_mean:\n",
      "    month_num: ΔMSE = 0.0752\n",
      "    o3_max1_hour: ΔMSE = 0.0399\n",
      "    ndvi: ΔMSE = 0.0301\n",
      "    o3_max1_value: ΔMSE = 0.0179\n",
      "    co_mean: ΔMSE = 0.0147\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: o3_max1_value\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target o3_max1_value:\n",
      "    o3_max1_hour: ΔMSE = 0.0893\n",
      "    month_num: ΔMSE = 0.0631\n",
      "    o3_aqi: ΔMSE = 0.0262\n",
      "    ndvi: ΔMSE = 0.0248\n",
      "    o3_mean: ΔMSE = 0.0205\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: o3_max1_hour\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target o3_max1_hour:\n",
      "    month_num: ΔMSE = 0.1284\n",
      "    vmt: ΔMSE = 0.0613\n",
      "    pm10_max1_value: ΔMSE = 0.0460\n",
      "    co_max1_hour: ΔMSE = 0.0251\n",
      "    pm10_mean: ΔMSE = 0.0189\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: o3_aqi\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target o3_aqi:\n",
      "    month_num: ΔMSE = 0.1105\n",
      "    o3_max1_hour: ΔMSE = 0.0659\n",
      "    ndvi: ΔMSE = 0.0359\n",
      "    tavg: ΔMSE = 0.0308\n",
      "    pm10_aqi: ΔMSE = 0.0196\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: awnd\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target awnd:\n",
      "    month_num: ΔMSE = 0.0449\n",
      "    pm10_mean: ΔMSE = 0.0183\n",
      "    flights: ΔMSE = 0.0139\n",
      "    pm25_aqi: ΔMSE = 0.0133\n",
      "    prcp: ΔMSE = 0.0106\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: prcp\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target prcp:\n",
      "    pm25_aqi: ΔMSE = 0.0252\n",
      "    ndvi: ΔMSE = 0.0209\n",
      "    co_max1_hour: ΔMSE = 0.0169\n",
      "    month_num: ΔMSE = 0.0110\n",
      "    o3_max1_value: ΔMSE = 0.0073\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: tavg\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target tavg:\n",
      "    month_num: ΔMSE = 0.1175\n",
      "    tmax: ΔMSE = 0.0138\n",
      "    no2_max1_hour: ΔMSE = 0.0121\n",
      "    no2_aqi: ΔMSE = 0.0119\n",
      "    o3_mean: ΔMSE = 0.0111\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: tmax\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target tmax:\n",
      "    month_num: ΔMSE = 0.2211\n",
      "    o3_mean: ΔMSE = 0.0145\n",
      "    tavg: ΔMSE = 0.0107\n",
      "    vmt: ΔMSE = 0.0103\n",
      "    tmin: ΔMSE = 0.0093\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: tmin\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 5 features for target tmin:\n",
      "    month_num: ΔMSE = 0.1907\n",
      "    o3_mean: ΔMSE = 0.0133\n",
      "    tmax: ΔMSE = 0.0120\n",
      "    tavg: ΔMSE = 0.0097\n",
      "    vmt: ΔMSE = 0.0065\n",
      "\n",
      "======================================================================\n",
      "Training LSTM with target: month_num\n",
      "======================================================================\n",
      "  Top 5 features for target month_num:\n",
      "    prcp: ΔMSE = 0.0140\n",
      "    so2_max1_value: ΔMSE = 0.0094\n",
      "    tavg: ΔMSE = 0.0071\n",
      "    tmin: ΔMSE = 0.0039\n",
      "    tmax: ΔMSE = 0.0033\n",
      "\n",
      "=== Features most frequently appearing in top-5 across all targets ===\n",
      "            feature  count_in_topk\n",
      "0         month_num             21\n",
      "1              ndvi             12\n",
      "2     o3_max1_value             11\n",
      "3           o3_mean             10\n",
      "4            o3_aqi              9\n",
      "5              tavg              7\n",
      "6               vmt              7\n",
      "7          pm25_aqi              6\n",
      "8           flights              6\n",
      "9    no2_max1_value              5\n",
      "10     o3_max1_hour              5\n",
      "11             tmax              5\n",
      "12   pm25_max1_hour              4\n",
      "13          no2_aqi              4\n",
      "14         so2_mean              4\n",
      "15        pm10_mean              4\n",
      "16             awnd              4\n",
      "17         pm10_aqi              4\n",
      "18           co_aqi              3\n",
      "19   pm10_max1_hour              3\n",
      "20          co_mean              3\n",
      "21     co_max1_hour              3\n",
      "22         no2_mean              3\n",
      "23             tmin              3\n",
      "24             prcp              3\n",
      "25   so2_max1_value              2\n",
      "26    co_max1_value              2\n",
      "27  pm10_max1_value              2\n",
      "28  pm25_max1_value              2\n",
      "29    so2_max1_hour              1\n",
      "30          so2_aqi              1\n",
      "31    no2_max1_hour              1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import Counter\n",
    "\n",
    "def build_lstm_model(n_timesteps, n_features, l2_lambda=1e-4):\n",
    "    l2_reg = regularizers.l2(l2_lambda)\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(n_timesteps, n_features)),\n",
    "        layers.LSTM(64, return_sequences=True,\n",
    "                    kernel_regularizer=l2_reg,\n",
    "                    recurrent_regularizer=l2_reg),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32, return_sequences=False,\n",
    "                    kernel_regularizer=l2_reg,\n",
    "                    recurrent_regularizer=l2_reg),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation=\"relu\", kernel_regularizer=l2_reg),\n",
    "        layers.Dense(32, activation=\"relu\", kernel_regularizer=l2_reg),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def permutation_importance_lstm(model, X_val, y_val, feature_cols):\n",
    "    \"\"\"\n",
    "    Returns dict: {feature_name: ΔMSE} for validation set.\n",
    "    \"\"\"\n",
    "    # Baseline performance\n",
    "    y_pred = model.predict(X_val, verbose=0)\n",
    "    baseline_mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "    importances = {}\n",
    "\n",
    "    for j, fname in enumerate(feature_cols):\n",
    "        X_perm = X_val.copy()\n",
    "        for t in range(X_perm.shape[1]):\n",
    "            np.random.shuffle(X_perm[:, t, j])\n",
    "\n",
    "        y_pred_perm = model.predict(X_perm, verbose=0)\n",
    "        mse_perm = mean_squared_error(y_val, y_pred_perm)\n",
    "\n",
    "        importances[fname] = mse_perm - baseline_mse\n",
    "\n",
    "    return importances, baseline_mse\n",
    "\n",
    "top_k = 5  \n",
    "feature_counter = Counter()\n",
    "all_target_importances = {}  #\n",
    "\n",
    "for target_col in feature_cols:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Training LSTM with target: {target_col}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Build sequences for this target\n",
    "    train_X, train_y = build_sequences(train_df_s, seq_len, feature_cols, target_col)\n",
    "    val_X,   val_y   = build_sequences(val_df_s,   seq_len, feature_cols, target_col)\n",
    "\n",
    "    # Skip if not enough data\n",
    "    if len(train_X) == 0 or len(val_X) == 0:\n",
    "        print(f\"  [SKIP] Not enough sequences for target {target_col}\")\n",
    "        continue\n",
    "\n",
    "    n_timesteps = train_X.shape[1]\n",
    "    n_features  = train_X.shape[2]\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_lstm_model(n_timesteps, n_features, l2_lambda=1e-4)\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_X, train_y,\n",
    "        validation_data=(val_X, val_y),\n",
    "        epochs=150,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0  \n",
    "    )\n",
    "\n",
    "    # Permutation importance for this target\n",
    "    importances, baseline_mse = permutation_importance_lstm(\n",
    "        model, val_X, val_y, feature_cols\n",
    "    )\n",
    "    all_target_importances[target_col] = importances\n",
    "\n",
    "    # Sort features by MSE (descending)\n",
    "    sorted_imps = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # exclude the target itself from the ranking:\n",
    "    sorted_imps = [item for item in sorted_imps if item[0] != target_col]\n",
    "\n",
    "    # Take top-k important features for this target\n",
    "    top_feats_for_target = [f for f, imp in sorted_imps[:top_k]]\n",
    "\n",
    "    print(f\"  Top {top_k} features for target {target_col}:\")\n",
    "    for f, imp in sorted_imps[:top_k]:\n",
    "        print(f\"    {f}: ΔMSE = {imp:.4f}\")\n",
    "\n",
    "    feature_counter.update(top_feats_for_target)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"feature\": list(feature_counter.keys()),\n",
    "    \"count_in_topk\": list(feature_counter.values())\n",
    "}).sort_values(\"count_in_topk\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Features most frequently appearing in top-{} across all targets ===\".format(top_k))\n",
    "print(summary_df)\n",
    "\n",
    "summary_df.to_csv(\"lstm_permutation_global_importance_topk.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f497d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 2.5730667114257812\n",
      "Test MAE: 1.1804025173187256\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 9.9267 - mae: 1.7543\n",
      "Validation MAE: 1.7542928457260132\n",
      "19/19 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mae = model.evaluate(test_X, test_y, verbose=0)\n",
    "print(\"Test MSE:\", test_loss)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "\n",
    "val_loss, val_mae = model.evaluate(val_X, val_y)\n",
    "print(\"Validation MAE:\", val_mae)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c060127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.20\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ddd5d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoregressive feature cols: ['pm25_mean', 'month_sin', 'month_cos']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Build AR dataset: pm25_mean + selected features\n",
    "\n",
    "df_ar = full.copy()  \n",
    "df_ar[\"month\"] = df_ar[\"month\"].dt.to_timestamp()\n",
    "df_ar = df_ar.sort_values([\"state\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "target_col = \"pm25_mean\"\n",
    "\n",
    "# Time features\n",
    "df_ar[\"month_num\"] = df_ar[\"month\"].dt.month\n",
    "df_ar[\"month_sin\"] = np.sin(2 * np.pi * df_ar[\"month_num\"] / 12)\n",
    "df_ar[\"month_cos\"] = np.cos(2 * np.pi * df_ar[\"month_num\"] / 12)\n",
    "\n",
    "ar_feature_cols = [\"pm25_mean\", \"month_sin\", \"month_cos\"]\n",
    "print(\"Autoregressive feature cols:\", ar_feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20aded37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR Train: 2018-01-01 00:00:00 -> 2021-12-01 00:00:00\n",
      "AR Val:   2022-01-01 00:00:00 -> 2023-12-01 00:00:00\n",
      "AR Test:  2024-01-01 00:00:00 -> 2025-07-01 00:00:00\n",
      "AR train: (1800, 12, 3) (1800,)\n",
      "AR val:   (600, 12, 3) (600,)\n",
      "AR test:  (350, 12, 3) (350,)\n"
     ]
    }
   ],
   "source": [
    "train_end = pd.to_datetime(\"2021-12-01\")\n",
    "val_end   = pd.to_datetime(\"2023-12-01\")\n",
    "\n",
    "train_df_ar = df_ar[df_ar[\"month\"] <= train_end].copy()\n",
    "val_df_ar   = df_ar[(df_ar[\"month\"] > train_end) & (df_ar[\"month\"] <= val_end)].copy()\n",
    "test_df_ar  = df_ar[df_ar[\"month\"] > val_end].copy()\n",
    "\n",
    "print(\"AR Train:\", train_df_ar[\"month\"].min(), \"->\", train_df_ar[\"month\"].max())\n",
    "print(\"AR Val:  \", val_df_ar[\"month\"].min(),   \"->\", val_df_ar[\"month\"].max())\n",
    "print(\"AR Test: \", test_df_ar[\"month\"].min(),  \"->\", test_df_ar[\"month\"].max())\n",
    "\n",
    "\n",
    "# Build AR sequences: past 12 steps -> next step\n",
    "\n",
    "seq_len = 12\n",
    "\n",
    "def build_ar_sequences(df, seq_len, feature_cols, target_col):\n",
    "    X_list, y_list = [], []\n",
    "    for state in df[\"state\"].unique():\n",
    "        g = df[df[\"state\"] == state].sort_values(\"month\")\n",
    "        feat = g[feature_cols].values   \n",
    "        targ = g[target_col].values \n",
    "        if len(g) <= seq_len:\n",
    "            continue\n",
    "        for i in range(len(g) - seq_len):\n",
    "            X_list.append(feat[i:i+seq_len])\n",
    "            y_list.append(targ[i+seq_len])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "train_X_ar, train_y_ar = build_ar_sequences(train_df_ar, seq_len, ar_feature_cols, target_col)\n",
    "val_X_ar,   val_y_ar   = build_ar_sequences(val_df_ar,   seq_len, ar_feature_cols, target_col)\n",
    "test_X_ar,  test_y_ar  = build_ar_sequences(test_df_ar,  seq_len, ar_feature_cols, target_col)\n",
    "\n",
    "print(\"AR train:\", train_X_ar.shape, train_y_ar.shape)\n",
    "print(\"AR val:  \", val_X_ar.shape,   val_y_ar.shape)\n",
    "print(\"AR test: \", test_X_ar.shape,  test_y_ar.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f368b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total targets to predict: 31\n"
     ]
    }
   ],
   "source": [
    "all_target_cols = [\n",
    "    'vmt', 'ndvi', 'flights',\n",
    "    'co_mean', 'co_max1_value', 'co_max1_hour', 'co_aqi',\n",
    "    'no2_mean', 'no2_max1_value', 'no2_max1_hour', 'no2_aqi',\n",
    "    'pm25_max1_value', 'pm25_max1_hour', 'pm25_aqi',\n",
    "    'pm10_mean', 'pm10_max1_value', 'pm10_max1_hour', 'pm10_aqi',\n",
    "    'so2_mean', 'so2_max1_value', 'so2_max1_hour', 'so2_aqi',\n",
    "    'o3_mean', 'o3_max1_value', 'o3_max1_hour', 'o3_aqi',\n",
    "    'awnd', 'prcp', 'tavg', 'tmax', 'tmin'\n",
    "]\n",
    "\n",
    "print(f\"Total targets to predict: {len(all_target_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7f3bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All targets to forecast: ['pm25_mean', 'vmt', 'ndvi', 'flights', 'co_mean', 'co_max1_value', 'co_max1_hour', 'co_aqi', 'no2_mean', 'no2_max1_value', 'no2_max1_hour', 'no2_aqi', 'pm25_max1_value', 'pm25_max1_hour', 'pm25_aqi', 'pm10_mean', 'pm10_max1_value', 'pm10_max1_hour', 'pm10_aqi', 'so2_mean', 'so2_max1_value', 'so2_max1_hour', 'so2_aqi', 'o3_mean', 'o3_max1_value', 'o3_max1_hour', 'o3_aqi', 'awnd', 'prcp', 'tavg', 'tmax', 'tmin']\n",
      "Top exogenous features (to be forecasted first): ['ndvi', 'o3_max1_value', 'o3_mean', 'o3_aqi', 'tavg']\n",
      "Train period: 2018-01-01 00:00:00 -> 2021-12-01 00:00:00\n",
      "Val period:   2022-01-01 00:00:00 -> 2023-12-01 00:00:00\n",
      "Test period:  2024-01-01 00:00:00 -> 2025-07-01 00:00:00\n",
      "\n",
      "======================================================================\n",
      "Stage 1: Univariate AR LSTM for exogenous target: ndvi\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 4), val(600, 12, 4), test(350, 12, 4)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 2s 9ms/step - loss: 0.0768 - mae: 0.1927 - val_loss: 0.0526 - val_mae: 0.1472 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0456 - mae: 0.1319 - val_loss: 0.0328 - val_mae: 0.1001 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0307 - mae: 0.0901 - val_loss: 0.0257 - val_mae: 0.0751 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0256 - mae: 0.0766 - val_loss: 0.0219 - val_mae: 0.0636 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0228 - mae: 0.0703 - val_loss: 0.0226 - val_mae: 0.0758 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0212 - mae: 0.0682 - val_loss: 0.0206 - val_mae: 0.0712 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0193 - mae: 0.0649 - val_loss: 0.0202 - val_mae: 0.0755 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0624 - val_loss: 0.0185 - val_mae: 0.0695 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0609 - val_loss: 0.0197 - val_mae: 0.0809 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0607 - val_loss: 0.0181 - val_mae: 0.0772 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0151 - mae: 0.0598 - val_loss: 0.0143 - val_mae: 0.0583 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0144 - mae: 0.0597 - val_loss: 0.0173 - val_mae: 0.0799 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0134 - mae: 0.0570 - val_loss: 0.0142 - val_mae: 0.0646 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0529 - val_loss: 0.0155 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0116 - mae: 0.0518 - val_loss: 0.0155 - val_mae: 0.0779 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0517 - val_loss: 0.0119 - val_mae: 0.0582 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0509 - val_loss: 0.0112 - val_mae: 0.0559 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0102 - mae: 0.0498 - val_loss: 0.0122 - val_mae: 0.0652 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0099 - mae: 0.0495 - val_loss: 0.0116 - val_mae: 0.0629 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0093 - mae: 0.0476 - val_loss: 0.0120 - val_mae: 0.0675 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0475 - val_loss: 0.0096 - val_mae: 0.0527 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0094 - mae: 0.0515 - val_loss: 0.0150 - val_mae: 0.0857 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0086 - mae: 0.0480 - val_loss: 0.0102 - val_mae: 0.0600 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0477 - val_loss: 0.0104 - val_mae: 0.0622 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0085 - mae: 0.0493 - val_loss: 0.0106 - val_mae: 0.0659 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0079 - mae: 0.0464 - val_loss: 0.0094 - val_mae: 0.0583 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0074 - mae: 0.0448 - val_loss: 0.0122 - val_mae: 0.0776 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0073 - mae: 0.0454 - val_loss: 0.0105 - val_mae: 0.0676 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0071 - mae: 0.0452 - val_loss: 0.0089 - val_mae: 0.0584 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0070 - mae: 0.0445 - val_loss: 0.0093 - val_mae: 0.0625 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0070 - mae: 0.0459 - val_loss: 0.0087 - val_mae: 0.0595 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0065 - mae: 0.0430 - val_loss: 0.0077 - val_mae: 0.0518 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0064 - mae: 0.0434 - val_loss: 0.0131 - val_mae: 0.0852 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0066 - mae: 0.0463 - val_loss: 0.0115 - val_mae: 0.0796 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0062 - mae: 0.0430 - val_loss: 0.0069 - val_mae: 0.0479 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0063 - mae: 0.0438 - val_loss: 0.0091 - val_mae: 0.0671 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0059 - mae: 0.0423 - val_loss: 0.0070 - val_mae: 0.0486 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0060 - mae: 0.0435 - val_loss: 0.0064 - val_mae: 0.0467 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0064 - mae: 0.0477 - val_loss: 0.0072 - val_mae: 0.0544 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0057 - mae: 0.0423 - val_loss: 0.0061 - val_mae: 0.0459 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0060 - mae: 0.0457 - val_loss: 0.0095 - val_mae: 0.0702 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0054 - mae: 0.0411 - val_loss: 0.0063 - val_mae: 0.0484 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0053 - mae: 0.0406 - val_loss: 0.0068 - val_mae: 0.0524 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0052 - mae: 0.0407 - val_loss: 0.0068 - val_mae: 0.0533 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0052 - mae: 0.0414 - val_loss: 0.0057 - val_mae: 0.0454 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0052 - mae: 0.0418 - val_loss: 0.0065 - val_mae: 0.0520 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0048 - mae: 0.0395 - val_loss: 0.0064 - val_mae: 0.0525 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0051 - mae: 0.0413 - val_loss: 0.0098 - val_mae: 0.0768 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0049 - mae: 0.0409 - val_loss: 0.0063 - val_mae: 0.0518 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0048 - mae: 0.0404 - val_loss: 0.0086 - val_mae: 0.0674 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0046 - mae: 0.0390 - val_loss: 0.0063 - val_mae: 0.0535 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0045 - mae: 0.0382 - val_loss: 0.0079 - val_mae: 0.0638 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0044 - mae: 0.0382 - val_loss: 0.0058 - val_mae: 0.0496 - lr: 5.0000e-04\n",
      "  ndvi - Test MSE: 0.0051, Test MAE: 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 1: Univariate AR LSTM for exogenous target: o3_max1_value\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 4), val(600, 12, 4), test(350, 12, 4)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.0256 - mae: 0.0417 - val_loss: 0.0193 - val_mae: 0.0114 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0158 - val_loss: 0.0159 - val_mae: 0.0077 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0148 - mae: 0.0118 - val_loss: 0.0133 - val_mae: 0.0088 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0124 - mae: 0.0093 - val_loss: 0.0113 - val_mae: 0.0086 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0081 - val_loss: 0.0097 - val_mae: 0.0079 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0075 - val_loss: 0.0085 - val_mae: 0.0084 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0079 - mae: 0.0067 - val_loss: 0.0074 - val_mae: 0.0076 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0070 - mae: 0.0066 - val_loss: 0.0066 - val_mae: 0.0077 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0062 - mae: 0.0060 - val_loss: 0.0059 - val_mae: 0.0067 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0055 - mae: 0.0058 - val_loss: 0.0053 - val_mae: 0.0071 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0050 - mae: 0.0056 - val_loss: 0.0047 - val_mae: 0.0067 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0045 - mae: 0.0054 - val_loss: 0.0043 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0041 - mae: 0.0054 - val_loss: 0.0039 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0037 - mae: 0.0053 - val_loss: 0.0036 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0034 - mae: 0.0054 - val_loss: 0.0033 - val_mae: 0.0061 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0031 - mae: 0.0051 - val_loss: 0.0030 - val_mae: 0.0054 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0029 - mae: 0.0053 - val_loss: 0.0028 - val_mae: 0.0055 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0027 - mae: 0.0052 - val_loss: 0.0026 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0025 - mae: 0.0052 - val_loss: 0.0024 - val_mae: 0.0055 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0023 - mae: 0.0053 - val_loss: 0.0022 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0021 - mae: 0.0050 - val_loss: 0.0020 - val_mae: 0.0055 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0020 - mae: 0.0050 - val_loss: 0.0019 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0018 - mae: 0.0050 - val_loss: 0.0017 - val_mae: 0.0054 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0017 - mae: 0.0051 - val_loss: 0.0016 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0016 - mae: 0.0051 - val_loss: 0.0015 - val_mae: 0.0053 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0015 - mae: 0.0051 - val_loss: 0.0014 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0014 - mae: 0.0050 - val_loss: 0.0013 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0013 - mae: 0.0049 - val_loss: 0.0012 - val_mae: 0.0061 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0012 - mae: 0.0050 - val_loss: 0.0011 - val_mae: 0.0053 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0011 - mae: 0.0051 - val_loss: 0.0011 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0010 - mae: 0.0051 - val_loss: 9.9830e-04 - val_mae: 0.0054 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.6064e-04 - mae: 0.0049 - val_loss: 9.3393e-04 - val_mae: 0.0055 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.9948e-04 - mae: 0.0051 - val_loss: 8.7394e-04 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.3825e-04 - mae: 0.0049 - val_loss: 8.2643e-04 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8518e-04 - mae: 0.0050 - val_loss: 7.6439e-04 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3238e-04 - mae: 0.0050 - val_loss: 7.2560e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.8640e-04 - mae: 0.0050 - val_loss: 6.7336e-04 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.4249e-04 - mae: 0.0051 - val_loss: 6.2524e-04 - val_mae: 0.0054 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.0318e-04 - mae: 0.0052 - val_loss: 5.8642e-04 - val_mae: 0.0055 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.6110e-04 - mae: 0.0049 - val_loss: 5.4777e-04 - val_mae: 0.0054 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.2757e-04 - mae: 0.0052 - val_loss: 5.1494e-04 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.9377e-04 - mae: 0.0051 - val_loss: 4.8523e-04 - val_mae: 0.0058 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.6179e-04 - mae: 0.0051 - val_loss: 4.4965e-04 - val_mae: 0.0054 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.3237e-04 - mae: 0.0050 - val_loss: 4.2185e-04 - val_mae: 0.0054 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.0370e-04 - mae: 0.0049 - val_loss: 3.9833e-04 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.7850e-04 - mae: 0.0050 - val_loss: 3.8184e-04 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.5520e-04 - mae: 0.0050 - val_loss: 3.5487e-04 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.3262e-04 - mae: 0.0050 - val_loss: 3.3019e-04 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.1359e-04 - mae: 0.0051 - val_loss: 3.0942e-04 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.9382e-04 - mae: 0.0050 - val_loss: 3.0254e-04 - val_mae: 0.0064 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.7831e-04 - mae: 0.0049 - val_loss: 2.8015e-04 - val_mae: 0.0055 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.6905e-04 - mae: 0.0049 - val_loss: 2.7832e-04 - val_mae: 0.0060 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.6108e-04 - mae: 0.0049 - val_loss: 2.6184e-04 - val_mae: 0.0053 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.5460e-04 - mae: 0.0050 - val_loss: 2.5388e-04 - val_mae: 0.0054 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4416e-04 - mae: 0.0049 - val_loss: 2.5594e-04 - val_mae: 0.0061 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.3591e-04 - mae: 0.0049 - val_loss: 2.4457e-04 - val_mae: 0.0059 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2892e-04 - mae: 0.0048 - val_loss: 2.3960e-04 - val_mae: 0.0058 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2472e-04 - mae: 0.0048 - val_loss: 2.3166e-04 - val_mae: 0.0055 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2270e-04 - mae: 0.0049 - val_loss: 2.2681e-04 - val_mae: 0.0055 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1798e-04 - mae: 0.0049 - val_loss: 2.2697e-04 - val_mae: 0.0058 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1378e-04 - mae: 0.0049 - val_loss: 2.1768e-04 - val_mae: 0.0053 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1035e-04 - mae: 0.0048 - val_loss: 2.1722e-04 - val_mae: 0.0055 - lr: 1.2500e-04\n",
      "Epoch 63/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0850e-04 - mae: 0.0048 - val_loss: 2.1754e-04 - val_mae: 0.0057 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0692e-04 - mae: 0.0049 - val_loss: 2.1539e-04 - val_mae: 0.0057 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0353e-04 - mae: 0.0048 - val_loss: 2.1176e-04 - val_mae: 0.0055 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0201e-04 - mae: 0.0048 - val_loss: 2.1154e-04 - val_mae: 0.0057 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0094e-04 - mae: 0.0049 - val_loss: 2.0723e-04 - val_mae: 0.0054 - lr: 6.2500e-05\n",
      "Epoch 68/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9888e-04 - mae: 0.0048 - val_loss: 2.0979e-04 - val_mae: 0.0057 - lr: 6.2500e-05\n",
      "Epoch 69/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9806e-04 - mae: 0.0048 - val_loss: 2.0959e-04 - val_mae: 0.0058 - lr: 6.2500e-05\n",
      "Epoch 70/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9766e-04 - mae: 0.0048 - val_loss: 2.0529e-04 - val_mae: 0.0055 - lr: 6.2500e-05\n",
      "Epoch 71/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9623e-04 - mae: 0.0048 - val_loss: 2.0821e-04 - val_mae: 0.0058 - lr: 6.2500e-05\n",
      "Epoch 72/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9552e-04 - mae: 0.0048 - val_loss: 2.0549e-04 - val_mae: 0.0057 - lr: 3.1250e-05\n",
      "Epoch 73/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9434e-04 - mae: 0.0048 - val_loss: 2.0417e-04 - val_mae: 0.0056 - lr: 3.1250e-05\n",
      "Epoch 74/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9409e-04 - mae: 0.0048 - val_loss: 2.0477e-04 - val_mae: 0.0057 - lr: 3.1250e-05\n",
      "Epoch 75/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9294e-04 - mae: 0.0048 - val_loss: 2.0182e-04 - val_mae: 0.0056 - lr: 3.1250e-05\n",
      "Epoch 76/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9347e-04 - mae: 0.0048 - val_loss: 2.0306e-04 - val_mae: 0.0057 - lr: 3.1250e-05\n",
      "Epoch 77/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9223e-04 - mae: 0.0048 - val_loss: 2.0233e-04 - val_mae: 0.0057 - lr: 1.5625e-05\n",
      "Epoch 78/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9146e-04 - mae: 0.0048 - val_loss: 2.0129e-04 - val_mae: 0.0056 - lr: 1.5625e-05\n",
      "Epoch 79/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9099e-04 - mae: 0.0048 - val_loss: 2.0194e-04 - val_mae: 0.0057 - lr: 1.5625e-05\n",
      "Epoch 80/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9123e-04 - mae: 0.0048 - val_loss: 2.0199e-04 - val_mae: 0.0057 - lr: 1.5625e-05\n",
      "Epoch 81/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9016e-04 - mae: 0.0047 - val_loss: 2.0104e-04 - val_mae: 0.0057 - lr: 1.5625e-05\n",
      "Epoch 82/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9062e-04 - mae: 0.0048 - val_loss: 1.9996e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9040e-04 - mae: 0.0048 - val_loss: 2.0046e-04 - val_mae: 0.0057 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8914e-04 - mae: 0.0047 - val_loss: 1.9861e-04 - val_mae: 0.0055 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9030e-04 - mae: 0.0048 - val_loss: 1.9873e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8907e-04 - mae: 0.0048 - val_loss: 1.9912e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8908e-04 - mae: 0.0048 - val_loss: 1.9781e-04 - val_mae: 0.0055 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8912e-04 - mae: 0.0048 - val_loss: 1.9938e-04 - val_mae: 0.0057 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8843e-04 - mae: 0.0048 - val_loss: 1.9819e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8865e-04 - mae: 0.0048 - val_loss: 1.9728e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8738e-04 - mae: 0.0048 - val_loss: 1.9680e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8828e-04 - mae: 0.0048 - val_loss: 1.9756e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8752e-04 - mae: 0.0048 - val_loss: 1.9650e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8725e-04 - mae: 0.0048 - val_loss: 1.9837e-04 - val_mae: 0.0057 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8688e-04 - mae: 0.0048 - val_loss: 1.9573e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8660e-04 - mae: 0.0048 - val_loss: 1.9576e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8565e-04 - mae: 0.0048 - val_loss: 1.9560e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8554e-04 - mae: 0.0048 - val_loss: 1.9427e-04 - val_mae: 0.0055 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8514e-04 - mae: 0.0048 - val_loss: 1.9549e-04 - val_mae: 0.0057 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8496e-04 - mae: 0.0048 - val_loss: 1.9388e-04 - val_mae: 0.0056 - lr: 1.0000e-05\n",
      "  o3_max1_value - Test MSE: 0.0002, Test MAE: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 1: Univariate AR LSTM for exogenous target: o3_mean\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 4), val(600, 12, 4), test(350, 12, 4)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.0211 - mae: 0.0287 - val_loss: 0.0163 - val_mae: 0.0068 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0119 - val_loss: 0.0117 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0094 - val_loss: 0.0088 - val_mae: 0.0058 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0076 - val_loss: 0.0068 - val_mae: 0.0055 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0062 - mae: 0.0066 - val_loss: 0.0055 - val_mae: 0.0056 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0050 - mae: 0.0061 - val_loss: 0.0046 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0042 - mae: 0.0057 - val_loss: 0.0038 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0035 - mae: 0.0056 - val_loss: 0.0032 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0030 - mae: 0.0052 - val_loss: 0.0028 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0026 - mae: 0.0053 - val_loss: 0.0024 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0022 - mae: 0.0052 - val_loss: 0.0021 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0019 - mae: 0.0052 - val_loss: 0.0018 - val_mae: 0.0061 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0017 - mae: 0.0053 - val_loss: 0.0016 - val_mae: 0.0061 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0015 - mae: 0.0052 - val_loss: 0.0014 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0013 - mae: 0.0052 - val_loss: 0.0012 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0011 - mae: 0.0052 - val_loss: 0.0011 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.9384e-04 - mae: 0.0052 - val_loss: 9.4172e-04 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.7602e-04 - mae: 0.0052 - val_loss: 8.3021e-04 - val_mae: 0.0059 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.7365e-04 - mae: 0.0052 - val_loss: 7.3658e-04 - val_mae: 0.0060 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.8543e-04 - mae: 0.0053 - val_loss: 6.6740e-04 - val_mae: 0.0066 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.0786e-04 - mae: 0.0053 - val_loss: 5.8484e-04 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.3900e-04 - mae: 0.0053 - val_loss: 5.1911e-04 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.7857e-04 - mae: 0.0053 - val_loss: 4.6138e-04 - val_mae: 0.0061 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.2695e-04 - mae: 0.0054 - val_loss: 4.1421e-04 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.8030e-04 - mae: 0.0054 - val_loss: 3.6981e-04 - val_mae: 0.0062 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.3864e-04 - mae: 0.0054 - val_loss: 3.3492e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.0283e-04 - mae: 0.0054 - val_loss: 3.0266e-04 - val_mae: 0.0064 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.7153e-04 - mae: 0.0054 - val_loss: 2.7243e-04 - val_mae: 0.0064 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4423e-04 - mae: 0.0054 - val_loss: 2.4477e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1929e-04 - mae: 0.0055 - val_loss: 2.2039e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9756e-04 - mae: 0.0055 - val_loss: 1.9942e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7844e-04 - mae: 0.0054 - val_loss: 1.8496e-04 - val_mae: 0.0064 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6169e-04 - mae: 0.0055 - val_loss: 1.6652e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4701e-04 - mae: 0.0055 - val_loss: 1.5233e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3372e-04 - mae: 0.0055 - val_loss: 1.4050e-04 - val_mae: 0.0063 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2257e-04 - mae: 0.0055 - val_loss: 1.3193e-04 - val_mae: 0.0064 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1465e-04 - mae: 0.0055 - val_loss: 1.2819e-04 - val_mae: 0.0064 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0990e-04 - mae: 0.0055 - val_loss: 1.2203e-04 - val_mae: 0.0064 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0584e-04 - mae: 0.0055 - val_loss: 1.1863e-04 - val_mae: 0.0064 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0114e-04 - mae: 0.0055 - val_loss: 1.1324e-04 - val_mae: 0.0064 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.7114e-05 - mae: 0.0055 - val_loss: 1.0864e-04 - val_mae: 0.0064 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.4210e-05 - mae: 0.0055 - val_loss: 1.0727e-04 - val_mae: 0.0064 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.2334e-05 - mae: 0.0055 - val_loss: 1.0653e-04 - val_mae: 0.0064 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.0451e-05 - mae: 0.0055 - val_loss: 1.0457e-04 - val_mae: 0.0064 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.8721e-05 - mae: 0.0055 - val_loss: 1.0157e-04 - val_mae: 0.0064 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.6730e-05 - mae: 0.0055 - val_loss: 9.9601e-05 - val_mae: 0.0064 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.5345e-05 - mae: 0.0055 - val_loss: 9.8959e-05 - val_mae: 0.0064 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.4517e-05 - mae: 0.0055 - val_loss: 9.9441e-05 - val_mae: 0.0064 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.3644e-05 - mae: 0.0055 - val_loss: 9.7957e-05 - val_mae: 0.0064 - lr: 1.2500e-04\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.2797e-05 - mae: 0.0055 - val_loss: 9.7293e-05 - val_mae: 0.0064 - lr: 1.2500e-04\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.1962e-05 - mae: 0.0055 - val_loss: 9.6275e-05 - val_mae: 0.0064 - lr: 1.2500e-04\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.0956e-05 - mae: 0.0055 - val_loss: 9.4555e-05 - val_mae: 0.0064 - lr: 1.2500e-04\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.0246e-05 - mae: 0.0055 - val_loss: 9.4884e-05 - val_mae: 0.0064 - lr: 6.2500e-05\n",
      "Epoch 54/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9846e-05 - mae: 0.0055 - val_loss: 9.5145e-05 - val_mae: 0.0064 - lr: 6.2500e-05\n",
      "Epoch 55/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9431e-05 - mae: 0.0055 - val_loss: 9.4099e-05 - val_mae: 0.0064 - lr: 6.2500e-05\n",
      "Epoch 56/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8839e-05 - mae: 0.0055 - val_loss: 9.3524e-05 - val_mae: 0.0064 - lr: 6.2500e-05\n",
      "Epoch 57/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8510e-05 - mae: 0.0055 - val_loss: 9.3351e-05 - val_mae: 0.0064 - lr: 6.2500e-05\n",
      "Epoch 58/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8061e-05 - mae: 0.0055 - val_loss: 9.2784e-05 - val_mae: 0.0064 - lr: 3.1250e-05\n",
      "Epoch 59/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.7794e-05 - mae: 0.0055 - val_loss: 9.2439e-05 - val_mae: 0.0064 - lr: 3.1250e-05\n",
      "Epoch 60/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.7560e-05 - mae: 0.0055 - val_loss: 9.1887e-05 - val_mae: 0.0064 - lr: 3.1250e-05\n",
      "Epoch 61/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.7264e-05 - mae: 0.0055 - val_loss: 9.1911e-05 - val_mae: 0.0064 - lr: 3.1250e-05\n",
      "Epoch 62/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.7032e-05 - mae: 0.0055 - val_loss: 9.1778e-05 - val_mae: 0.0064 - lr: 3.1250e-05\n",
      "Epoch 63/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6786e-05 - mae: 0.0055 - val_loss: 9.1545e-05 - val_mae: 0.0064 - lr: 1.5625e-05\n",
      "Epoch 64/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6669e-05 - mae: 0.0055 - val_loss: 9.1491e-05 - val_mae: 0.0064 - lr: 1.5625e-05\n",
      "Epoch 65/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6551e-05 - mae: 0.0055 - val_loss: 9.1272e-05 - val_mae: 0.0064 - lr: 1.5625e-05\n",
      "Epoch 66/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6424e-05 - mae: 0.0055 - val_loss: 9.1222e-05 - val_mae: 0.0064 - lr: 1.5625e-05\n",
      "Epoch 67/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6287e-05 - mae: 0.0055 - val_loss: 9.0988e-05 - val_mae: 0.0064 - lr: 1.5625e-05\n",
      "Epoch 68/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6086e-05 - mae: 0.0055 - val_loss: 9.1054e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5992e-05 - mae: 0.0055 - val_loss: 9.0877e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5930e-05 - mae: 0.0055 - val_loss: 9.0684e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5790e-05 - mae: 0.0055 - val_loss: 9.0686e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5730e-05 - mae: 0.0055 - val_loss: 9.0679e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5583e-05 - mae: 0.0055 - val_loss: 9.0415e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5497e-05 - mae: 0.0055 - val_loss: 9.0331e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5410e-05 - mae: 0.0055 - val_loss: 9.0185e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5278e-05 - mae: 0.0055 - val_loss: 9.0143e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5145e-05 - mae: 0.0055 - val_loss: 8.9940e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.5053e-05 - mae: 0.0055 - val_loss: 8.9934e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4872e-05 - mae: 0.0055 - val_loss: 8.9756e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4794e-05 - mae: 0.0055 - val_loss: 8.9638e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4678e-05 - mae: 0.0055 - val_loss: 8.9453e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4495e-05 - mae: 0.0055 - val_loss: 8.9362e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4317e-05 - mae: 0.0055 - val_loss: 8.9211e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4248e-05 - mae: 0.0055 - val_loss: 8.9156e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4051e-05 - mae: 0.0055 - val_loss: 8.9041e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3939e-05 - mae: 0.0055 - val_loss: 8.8835e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3779e-05 - mae: 0.0055 - val_loss: 8.8764e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3672e-05 - mae: 0.0055 - val_loss: 8.8453e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3480e-05 - mae: 0.0055 - val_loss: 8.8365e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3322e-05 - mae: 0.0055 - val_loss: 8.8173e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3220e-05 - mae: 0.0055 - val_loss: 8.8035e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3035e-05 - mae: 0.0055 - val_loss: 8.7804e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.2790e-05 - mae: 0.0055 - val_loss: 8.7521e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.2639e-05 - mae: 0.0055 - val_loss: 8.7462e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.2464e-05 - mae: 0.0055 - val_loss: 8.7288e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.2276e-05 - mae: 0.0055 - val_loss: 8.7153e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.2090e-05 - mae: 0.0055 - val_loss: 8.6975e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.1912e-05 - mae: 0.0055 - val_loss: 8.6755e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.1758e-05 - mae: 0.0055 - val_loss: 8.6581e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.1515e-05 - mae: 0.0055 - val_loss: 8.6312e-05 - val_mae: 0.0064 - lr: 1.0000e-05\n",
      "  o3_mean - Test MSE: 0.0001, Test MAE: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 1: Univariate AR LSTM for exogenous target: o3_aqi\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 4), val(600, 12, 4), test(350, 12, 4)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1129.4797 - mae: 32.4524 - val_loss: 914.8085 - val_mae: 28.3510 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 300.2577 - mae: 14.2778 - val_loss: 113.3294 - val_mae: 8.3925 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 79.8153 - mae: 6.9538 - val_loss: 117.4951 - val_mae: 8.5082 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 77.5700 - mae: 6.8904 - val_loss: 108.8919 - val_mae: 8.1156 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 67.9675 - mae: 6.2870 - val_loss: 82.1819 - val_mae: 6.6509 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 45.4060 - mae: 4.9720 - val_loss: 46.2836 - val_mae: 4.6554 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 39.9793 - mae: 4.7123 - val_loss: 44.6472 - val_mae: 4.6264 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 36.7329 - mae: 4.5969 - val_loss: 38.2101 - val_mae: 4.3182 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 34.1325 - mae: 4.4125 - val_loss: 40.7335 - val_mae: 4.3998 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 31.2895 - mae: 4.2158 - val_loss: 40.5653 - val_mae: 4.3839 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 30.6697 - mae: 4.1291 - val_loss: 43.3352 - val_mae: 4.6160 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 31.0200 - mae: 4.1969 - val_loss: 30.1548 - val_mae: 3.8693 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 28.0583 - mae: 3.9473 - val_loss: 31.1901 - val_mae: 4.0188 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 28.0146 - mae: 4.0394 - val_loss: 37.7533 - val_mae: 4.2784 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26.2721 - mae: 3.8438 - val_loss: 31.4187 - val_mae: 3.9346 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 28.4579 - mae: 3.9832 - val_loss: 31.5951 - val_mae: 4.0662 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.9822 - mae: 3.8701 - val_loss: 34.0956 - val_mae: 4.0032 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.8828 - mae: 3.8707 - val_loss: 29.5057 - val_mae: 3.7887 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.8908 - mae: 3.8500 - val_loss: 30.5078 - val_mae: 3.8060 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.0791 - mae: 3.7822 - val_loss: 33.7156 - val_mae: 3.9240 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 24.8670 - mae: 3.7066 - val_loss: 37.9623 - val_mae: 4.1916 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 23.8884 - mae: 3.7211 - val_loss: 32.9500 - val_mae: 3.9521 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 23.6694 - mae: 3.6483 - val_loss: 36.3744 - val_mae: 4.1492 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 23.8063 - mae: 3.6920 - val_loss: 30.8851 - val_mae: 3.7928 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.2811 - mae: 3.8153 - val_loss: 32.6033 - val_mae: 3.8804 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 24.6939 - mae: 3.7485 - val_loss: 33.8173 - val_mae: 3.9366 - lr: 2.5000e-04\n",
      "  o3_aqi - Test MSE: 14.4815, Test MAE: 2.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 1: Univariate AR LSTM for exogenous target: tavg\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 4), val(600, 12, 4), test(350, 12, 4)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 172.9125 - mae: 10.8968 - val_loss: 83.6071 - val_mae: 7.8509 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 51.4150 - mae: 5.6380 - val_loss: 12.8568 - val_mae: 2.5761 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 14.3583 - mae: 2.7309 - val_loss: 6.9830 - val_mae: 1.7753 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.4487 - mae: 2.4164 - val_loss: 6.2324 - val_mae: 1.6905 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.9777 - mae: 2.3822 - val_loss: 5.6439 - val_mae: 1.6638 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.8909 - mae: 2.3013 - val_loss: 5.2209 - val_mae: 1.5854 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.9179 - mae: 2.2159 - val_loss: 6.0426 - val_mae: 1.7528 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.2639 - mae: 2.1751 - val_loss: 4.8612 - val_mae: 1.5534 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9624 - mae: 2.1387 - val_loss: 4.7424 - val_mae: 1.5392 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.4201 - mae: 2.1581 - val_loss: 4.8922 - val_mae: 1.6326 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4747 - mae: 2.0652 - val_loss: 4.3423 - val_mae: 1.4879 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3448 - mae: 2.0844 - val_loss: 4.4212 - val_mae: 1.5101 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.2244 - mae: 2.0624 - val_loss: 5.3358 - val_mae: 1.7862 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.0868 - mae: 2.0570 - val_loss: 4.6431 - val_mae: 1.5567 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.7933 - mae: 1.9976 - val_loss: 4.1756 - val_mae: 1.4767 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.6129 - mae: 1.9916 - val_loss: 4.4458 - val_mae: 1.5372 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.2657 - mae: 1.9659 - val_loss: 4.2757 - val_mae: 1.4987 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.0124 - mae: 1.9002 - val_loss: 4.0760 - val_mae: 1.4568 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.9899 - mae: 1.8957 - val_loss: 4.3725 - val_mae: 1.5429 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.3137 - mae: 1.9464 - val_loss: 4.5666 - val_mae: 1.5759 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.8757 - mae: 1.8694 - val_loss: 4.5320 - val_mae: 1.5572 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.7391 - mae: 1.8541 - val_loss: 4.8200 - val_mae: 1.6397 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.5378 - mae: 1.8364 - val_loss: 3.9502 - val_mae: 1.4420 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.3116 - mae: 1.7900 - val_loss: 4.4924 - val_mae: 1.5538 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.5432 - mae: 1.8362 - val_loss: 4.6277 - val_mae: 1.5691 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.6437 - mae: 1.8374 - val_loss: 4.4819 - val_mae: 1.5240 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.3366 - mae: 1.8038 - val_loss: 4.7185 - val_mae: 1.6129 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.2465 - mae: 1.7778 - val_loss: 4.7248 - val_mae: 1.6196 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.9102 - mae: 1.7373 - val_loss: 4.0940 - val_mae: 1.4504 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.8700 - mae: 1.7269 - val_loss: 4.6976 - val_mae: 1.5983 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.8313 - mae: 1.7190 - val_loss: 4.2362 - val_mae: 1.4941 - lr: 5.0000e-04\n",
      "  tavg - Test MSE: 5.5243, Test MAE: 1.6769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm25_mean\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 28.5046 - mae: 3.8284 - val_loss: 12.8522 - val_mae: 2.2613 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 12.4024 - mae: 2.0210 - val_loss: 12.9303 - val_mae: 2.2330 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.8945 - mae: 1.9149 - val_loss: 11.4648 - val_mae: 2.1363 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.1659 - mae: 1.8062 - val_loss: 11.4042 - val_mae: 1.9843 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.8900 - mae: 1.7417 - val_loss: 11.5780 - val_mae: 1.9665 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.3685 - mae: 1.7057 - val_loss: 11.1563 - val_mae: 1.8911 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.0402 - mae: 1.6393 - val_loss: 10.2869 - val_mae: 1.7897 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.3583 - mae: 1.5423 - val_loss: 9.5292 - val_mae: 1.7431 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.3831 - mae: 1.5804 - val_loss: 10.8462 - val_mae: 1.8885 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.2361 - mae: 1.5783 - val_loss: 10.7252 - val_mae: 1.8736 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.9922 - mae: 1.5287 - val_loss: 9.7582 - val_mae: 1.8039 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.9023 - mae: 1.4964 - val_loss: 9.6151 - val_mae: 1.7848 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.0054 - mae: 1.5577 - val_loss: 9.6036 - val_mae: 1.8073 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.5161 - mae: 1.4788 - val_loss: 9.7348 - val_mae: 1.8350 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.5013 - mae: 1.5124 - val_loss: 11.2492 - val_mae: 1.9877 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.3575 - mae: 1.4464 - val_loss: 9.4933 - val_mae: 1.8337 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.5755 - mae: 1.4622 - val_loss: 9.7940 - val_mae: 1.8423 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.0881 - mae: 1.4380 - val_loss: 9.9282 - val_mae: 1.8505 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.1135 - mae: 1.4709 - val_loss: 9.7957 - val_mae: 1.8709 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9732 - mae: 1.4526 - val_loss: 10.2519 - val_mae: 1.8981 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8927 - mae: 1.4662 - val_loss: 11.0248 - val_mae: 2.0042 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9426 - mae: 1.4637 - val_loss: 10.3564 - val_mae: 1.9128 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9397 - mae: 1.4155 - val_loss: 10.3088 - val_mae: 1.9105 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8237 - mae: 1.4197 - val_loss: 10.8173 - val_mae: 1.9576 - lr: 2.5000e-04\n",
      "  pm25_mean - Test MSE: 2.8838, Test MAE: 1.2959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: vmt\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 54394848.0000 - mae: 5123.1016 - val_loss: 58936912.0000 - val_mae: 5379.2852 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 54063304.0000 - mae: 5090.9883 - val_loss: 58296204.0000 - val_mae: 5319.4082 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 53076400.0000 - mae: 4992.2856 - val_loss: 56758816.0000 - val_mae: 5172.8813 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 51023596.0000 - mae: 4784.3906 - val_loss: 53860280.0000 - val_mae: 4886.7661 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 47614212.0000 - mae: 4442.5415 - val_loss: 49424252.0000 - val_mae: 4486.4692 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 42848692.0000 - mae: 4051.8596 - val_loss: 43895432.0000 - val_mae: 4098.3169 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 37395644.0000 - mae: 3535.6223 - val_loss: 37553556.0000 - val_mae: 3362.8403 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 31556018.0000 - mae: 2910.9241 - val_loss: 31475292.0000 - val_mae: 2813.0393 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26544548.0000 - mae: 2485.1116 - val_loss: 26480668.0000 - val_mae: 2381.6699 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 22970878.0000 - mae: 2235.1628 - val_loss: 22484282.0000 - val_mae: 1973.3848 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 19812386.0000 - mae: 2019.4646 - val_loss: 19857202.0000 - val_mae: 1925.2346 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17589624.0000 - mae: 1844.8521 - val_loss: 17253468.0000 - val_mae: 1689.6523 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15924453.0000 - mae: 1849.3652 - val_loss: 15058763.0000 - val_mae: 1484.4066 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 13421744.0000 - mae: 1593.6844 - val_loss: 13083773.0000 - val_mae: 1346.7700 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 12217219.0000 - mae: 1589.8055 - val_loss: 11498720.0000 - val_mae: 1308.8373 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11765375.0000 - mae: 1723.4443 - val_loss: 11113468.0000 - val_mae: 1689.4968 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 20518742.0000 - mae: 2637.4656 - val_loss: 20687558.0000 - val_mae: 2337.1016 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 16756045.0000 - mae: 1996.8627 - val_loss: 14713503.0000 - val_mae: 1487.2964 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 12930170.0000 - mae: 1700.6573 - val_loss: 11555902.0000 - val_mae: 1525.1694 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9996873.0000 - mae: 1652.4706 - val_loss: 12982086.0000 - val_mae: 1744.6251 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10192574.0000 - mae: 1785.0851 - val_loss: 6895774.5000 - val_mae: 1188.7689 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7411967.0000 - mae: 1501.2694 - val_loss: 9749647.0000 - val_mae: 2173.7253 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6166013.5000 - mae: 1435.9012 - val_loss: 5269217.0000 - val_mae: 1421.4526 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 16019443.0000 - mae: 2237.9443 - val_loss: 10351291.0000 - val_mae: 1814.2710 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6649966.0000 - mae: 1591.8873 - val_loss: 4067024.2500 - val_mae: 1111.7234 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5007719.0000 - mae: 1409.9271 - val_loss: 2662608.5000 - val_mae: 893.4222 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4955446.0000 - mae: 1419.2676 - val_loss: 4175009.2500 - val_mae: 1437.5846 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4545153.5000 - mae: 1406.5880 - val_loss: 2075680.8750 - val_mae: 851.4883 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6558819.0000 - mae: 1532.8077 - val_loss: 2791594.7500 - val_mae: 1208.6676 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8305643.0000 - mae: 1830.4775 - val_loss: 3055360.2500 - val_mae: 1099.2076 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 12770164.0000 - mae: 2113.5691 - val_loss: 2860260.2500 - val_mae: 1218.5754 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10747695.0000 - mae: 1904.3223 - val_loss: 14147144.0000 - val_mae: 1644.2035 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9840959.0000 - mae: 1775.0637 - val_loss: 4932810.5000 - val_mae: 1362.9801 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4679821.5000 - mae: 1407.4406 - val_loss: 2834483.7500 - val_mae: 1018.0346 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4076924.0000 - mae: 1330.8279 - val_loss: 2361990.7500 - val_mae: 1096.4026 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3493228.2500 - mae: 1243.0574 - val_loss: 1887757.5000 - val_mae: 901.9193 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3211741.0000 - mae: 1190.5168 - val_loss: 1651152.3750 - val_mae: 875.0197 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3311920.0000 - mae: 1214.4121 - val_loss: 1756316.3750 - val_mae: 885.7709 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3038183.0000 - mae: 1172.4878 - val_loss: 1321192.2500 - val_mae: 721.8667 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3246820.0000 - mae: 1202.0453 - val_loss: 1386191.5000 - val_mae: 735.1682 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3079644.5000 - mae: 1182.8639 - val_loss: 1225401.8750 - val_mae: 668.6713 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3170696.2500 - mae: 1171.2633 - val_loss: 1703148.5000 - val_mae: 870.4598 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3087477.0000 - mae: 1176.1246 - val_loss: 1336397.6250 - val_mae: 760.5824 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2844365.0000 - mae: 1141.1788 - val_loss: 1439545.2500 - val_mae: 795.9745 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 18959396.0000 - mae: 2509.9619 - val_loss: 15594837.0000 - val_mae: 1977.3137 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11185468.0000 - mae: 1818.6226 - val_loss: 3035238.5000 - val_mae: 959.6110 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5225398.0000 - mae: 1454.2386 - val_loss: 3731128.7500 - val_mae: 1245.1338 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4156717.7500 - mae: 1307.1654 - val_loss: 2603455.0000 - val_mae: 958.5870 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4227551.5000 - mae: 1351.8625 - val_loss: 2123842.2500 - val_mae: 878.5005 - lr: 2.5000e-04\n",
      "  vmt - Test MSE: 1526210.2500, Test MAE: 717.2828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: flights\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 1569493376.0000 - mae: 25324.9941 - val_loss: 1942523264.0000 - val_mae: 28686.3867 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1568290944.0000 - mae: 25301.7402 - val_loss: 1939986560.0000 - val_mae: 28642.5234 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1564584192.0000 - mae: 25228.5039 - val_loss: 1933617152.0000 - val_mae: 28532.6074 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1556711040.0000 - mae: 25072.5039 - val_loss: 1921379840.0000 - val_mae: 28321.7441 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1542837888.0000 - mae: 24803.6816 - val_loss: 1901568512.0000 - val_mae: 27976.4043 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1521517440.0000 - mae: 24376.8359 - val_loss: 1872480640.0000 - val_mae: 27463.7734 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1491527168.0000 - mae: 23800.8125 - val_loss: 1833129728.0000 - val_mae: 26781.5215 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1452367488.0000 - mae: 23072.0996 - val_loss: 1782452352.0000 - val_mae: 25993.9707 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1404351488.0000 - mae: 22293.3926 - val_loss: 1722044416.0000 - val_mae: 25202.8359 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1348388736.0000 - mae: 21550.9336 - val_loss: 1653930368.0000 - val_mae: 24405.4785 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1286625280.0000 - mae: 20895.3691 - val_loss: 1580447744.0000 - val_mae: 23825.7559 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1224961920.0000 - mae: 20414.6387 - val_loss: 1505543040.0000 - val_mae: 23404.6758 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1164432896.0000 - mae: 20096.8320 - val_loss: 1431391232.0000 - val_mae: 23029.2988 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1103424256.0000 - mae: 19822.2129 - val_loss: 1363160960.0000 - val_mae: 22722.9062 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1053330304.0000 - mae: 19700.0664 - val_loss: 1303701632.0000 - val_mae: 22592.6152 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1014569600.0000 - mae: 19730.0215 - val_loss: 1251723776.0000 - val_mae: 22418.6406 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 974913472.0000 - mae: 19814.6543 - val_loss: 1210288000.0000 - val_mae: 22401.6641 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 957789184.0000 - mae: 20132.8340 - val_loss: 1180981376.0000 - val_mae: 22483.3281 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 939773568.0000 - mae: 20214.7363 - val_loss: 1159126784.0000 - val_mae: 22604.9746 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 930145216.0000 - mae: 20558.4180 - val_loss: 1142643712.0000 - val_mae: 22742.2070 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 922175360.0000 - mae: 20757.1641 - val_loss: 1134284800.0000 - val_mae: 22837.3281 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 921193152.0000 - mae: 20861.9531 - val_loss: 1127924736.0000 - val_mae: 22923.5957 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 924187200.0000 - mae: 21074.1758 - val_loss: 1123504000.0000 - val_mae: 22995.8477 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 923713664.0000 - mae: 21167.0293 - val_loss: 1121851264.0000 - val_mae: 23026.6191 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 919691072.0000 - mae: 21208.0664 - val_loss: 1119163136.0000 - val_mae: 23080.7383 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 921781184.0000 - mae: 21310.9297 - val_loss: 1118873216.0000 - val_mae: 23087.0840 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 922233792.0000 - mae: 21302.7793 - val_loss: 1117567872.0000 - val_mae: 23116.5391 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 918727424.0000 - mae: 21350.3574 - val_loss: 1115915264.0000 - val_mae: 23158.1016 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 921518400.0000 - mae: 21371.3867 - val_loss: 1115659008.0000 - val_mae: 23164.9941 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 921787072.0000 - mae: 21388.0625 - val_loss: 1115516928.0000 - val_mae: 23168.8379 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 914683136.0000 - mae: 21307.6035 - val_loss: 1116103808.0000 - val_mae: 23153.0879 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 923592128.0000 - mae: 21511.4355 - val_loss: 1115910144.0000 - val_mae: 23158.2480 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 921227584.0000 - mae: 21343.2930 - val_loss: 1116540800.0000 - val_mae: 23141.7988 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 922903808.0000 - mae: 21366.4199 - val_loss: 1116806784.0000 - val_mae: 23135.0664 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 920956992.0000 - mae: 21358.7871 - val_loss: 1115939200.0000 - val_mae: 23157.4746 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 919410688.0000 - mae: 21368.3750 - val_loss: 1116129408.0000 - val_mae: 23152.4336 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 919287296.0000 - mae: 21349.5312 - val_loss: 1115897856.0000 - val_mae: 23158.5859 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 916144960.0000 - mae: 21320.7715 - val_loss: 1116304128.0000 - val_mae: 23147.8926 - lr: 5.0000e-04\n",
      "  flights - Test MSE: 1215036288.0000, Test MAE: 23753.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: co_mean\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.0464 - mae: 0.1118 - val_loss: 0.0279 - val_mae: 0.0771 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0271 - mae: 0.0771 - val_loss: 0.0237 - val_mae: 0.0736 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0231 - mae: 0.0706 - val_loss: 0.0214 - val_mae: 0.0722 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0212 - mae: 0.0692 - val_loss: 0.0206 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0199 - mae: 0.0694 - val_loss: 0.0219 - val_mae: 0.0835 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0196 - mae: 0.0713 - val_loss: 0.0187 - val_mae: 0.0739 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0672 - val_loss: 0.0182 - val_mae: 0.0745 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0660 - val_loss: 0.0159 - val_mae: 0.0686 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0164 - mae: 0.0653 - val_loss: 0.0166 - val_mae: 0.0719 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0159 - mae: 0.0654 - val_loss: 0.0164 - val_mae: 0.0735 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0656 - val_loss: 0.0154 - val_mae: 0.0701 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0147 - mae: 0.0630 - val_loss: 0.0144 - val_mae: 0.0665 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0143 - mae: 0.0635 - val_loss: 0.0144 - val_mae: 0.0681 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0608 - val_loss: 0.0135 - val_mae: 0.0653 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0623 - val_loss: 0.0124 - val_mae: 0.0620 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0582 - val_loss: 0.0120 - val_mae: 0.0600 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0126 - mae: 0.0601 - val_loss: 0.0146 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0119 - mae: 0.0577 - val_loss: 0.0112 - val_mae: 0.0586 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0526 - val_loss: 0.0135 - val_mae: 0.0705 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0505 - val_loss: 0.0097 - val_mae: 0.0514 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0503 - val_loss: 0.0097 - val_mae: 0.0542 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0097 - mae: 0.0503 - val_loss: 0.0097 - val_mae: 0.0532 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0091 - mae: 0.0467 - val_loss: 0.0093 - val_mae: 0.0535 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0096 - mae: 0.0515 - val_loss: 0.0102 - val_mae: 0.0568 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0090 - mae: 0.0478 - val_loss: 0.0085 - val_mae: 0.0494 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0084 - mae: 0.0448 - val_loss: 0.0094 - val_mae: 0.0582 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0087 - mae: 0.0477 - val_loss: 0.0099 - val_mae: 0.0577 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0081 - mae: 0.0443 - val_loss: 0.0080 - val_mae: 0.0481 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0438 - val_loss: 0.0083 - val_mae: 0.0508 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0449 - val_loss: 0.0077 - val_mae: 0.0479 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0080 - mae: 0.0467 - val_loss: 0.0075 - val_mae: 0.0486 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0083 - mae: 0.0485 - val_loss: 0.0077 - val_mae: 0.0486 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0466 - val_loss: 0.0081 - val_mae: 0.0518 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0075 - mae: 0.0453 - val_loss: 0.0129 - val_mae: 0.0791 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0077 - mae: 0.0471 - val_loss: 0.0079 - val_mae: 0.0518 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0078 - mae: 0.0480 - val_loss: 0.0076 - val_mae: 0.0518 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0070 - mae: 0.0430 - val_loss: 0.0071 - val_mae: 0.0477 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0068 - mae: 0.0423 - val_loss: 0.0079 - val_mae: 0.0522 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0069 - mae: 0.0430 - val_loss: 0.0070 - val_mae: 0.0489 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0070 - mae: 0.0432 - val_loss: 0.0073 - val_mae: 0.0494 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0068 - mae: 0.0424 - val_loss: 0.0078 - val_mae: 0.0528 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0068 - mae: 0.0428 - val_loss: 0.0088 - val_mae: 0.0588 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0068 - mae: 0.0436 - val_loss: 0.0073 - val_mae: 0.0500 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0065 - mae: 0.0413 - val_loss: 0.0068 - val_mae: 0.0472 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0065 - mae: 0.0416 - val_loss: 0.0069 - val_mae: 0.0481 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0065 - mae: 0.0418 - val_loss: 0.0066 - val_mae: 0.0467 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0064 - mae: 0.0417 - val_loss: 0.0067 - val_mae: 0.0474 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0066 - mae: 0.0436 - val_loss: 0.0076 - val_mae: 0.0529 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0064 - mae: 0.0417 - val_loss: 0.0067 - val_mae: 0.0483 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0063 - mae: 0.0414 - val_loss: 0.0064 - val_mae: 0.0469 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0062 - mae: 0.0411 - val_loss: 0.0075 - val_mae: 0.0529 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0064 - mae: 0.0433 - val_loss: 0.0067 - val_mae: 0.0483 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0063 - mae: 0.0416 - val_loss: 0.0065 - val_mae: 0.0469 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0063 - mae: 0.0436 - val_loss: 0.0082 - val_mae: 0.0577 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0062 - mae: 0.0422 - val_loss: 0.0065 - val_mae: 0.0478 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0060 - mae: 0.0407 - val_loss: 0.0067 - val_mae: 0.0488 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0059 - mae: 0.0400 - val_loss: 0.0065 - val_mae: 0.0474 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0059 - mae: 0.0400 - val_loss: 0.0063 - val_mae: 0.0469 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0058 - mae: 0.0398 - val_loss: 0.0066 - val_mae: 0.0481 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0059 - mae: 0.0398 - val_loss: 0.0065 - val_mae: 0.0477 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0059 - mae: 0.0406 - val_loss: 0.0064 - val_mae: 0.0476 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0059 - mae: 0.0406 - val_loss: 0.0065 - val_mae: 0.0475 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0058 - mae: 0.0399 - val_loss: 0.0066 - val_mae: 0.0483 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0058 - mae: 0.0397 - val_loss: 0.0064 - val_mae: 0.0477 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0058 - mae: 0.0405 - val_loss: 0.0064 - val_mae: 0.0477 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0058 - mae: 0.0401 - val_loss: 0.0066 - val_mae: 0.0484 - lr: 1.2500e-04\n",
      "  co_mean - Test MSE: 0.0085, Test MAE: 0.0582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: co_max1_value\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.0509 - mae: 0.1277 - val_loss: 0.0376 - val_mae: 0.1091 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0405 - mae: 0.1121 - val_loss: 0.0329 - val_mae: 0.1048 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0360 - mae: 0.1078 - val_loss: 0.0319 - val_mae: 0.1044 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0334 - mae: 0.1023 - val_loss: 0.0290 - val_mae: 0.0986 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0316 - mae: 0.1014 - val_loss: 0.0310 - val_mae: 0.1073 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0296 - mae: 0.0968 - val_loss: 0.0259 - val_mae: 0.0945 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0273 - mae: 0.0936 - val_loss: 0.0247 - val_mae: 0.0947 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0258 - mae: 0.0903 - val_loss: 0.0248 - val_mae: 0.0982 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0268 - mae: 0.0939 - val_loss: 0.0221 - val_mae: 0.0883 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0238 - mae: 0.0865 - val_loss: 0.0249 - val_mae: 0.0969 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0225 - mae: 0.0839 - val_loss: 0.0218 - val_mae: 0.0884 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0209 - mae: 0.0776 - val_loss: 0.0218 - val_mae: 0.0887 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0201 - mae: 0.0759 - val_loss: 0.0237 - val_mae: 0.0975 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0196 - mae: 0.0752 - val_loss: 0.0195 - val_mae: 0.0830 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0727 - val_loss: 0.0200 - val_mae: 0.0861 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0741 - val_loss: 0.0164 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0182 - mae: 0.0741 - val_loss: 0.0163 - val_mae: 0.0758 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0692 - val_loss: 0.0154 - val_mae: 0.0738 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0708 - val_loss: 0.0171 - val_mae: 0.0792 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0682 - val_loss: 0.0151 - val_mae: 0.0737 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0743 - val_loss: 0.0148 - val_mae: 0.0722 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0675 - val_loss: 0.0166 - val_mae: 0.0787 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0695 - val_loss: 0.0136 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0157 - mae: 0.0701 - val_loss: 0.0142 - val_mae: 0.0742 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0718 - val_loss: 0.0137 - val_mae: 0.0697 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0150 - mae: 0.0670 - val_loss: 0.0131 - val_mae: 0.0704 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0148 - mae: 0.0672 - val_loss: 0.0146 - val_mae: 0.0745 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0150 - mae: 0.0690 - val_loss: 0.0146 - val_mae: 0.0755 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0149 - mae: 0.0696 - val_loss: 0.0151 - val_mae: 0.0777 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0144 - mae: 0.0673 - val_loss: 0.0133 - val_mae: 0.0716 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0144 - mae: 0.0679 - val_loss: 0.0128 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0145 - mae: 0.0703 - val_loss: 0.0181 - val_mae: 0.0918 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0658 - val_loss: 0.0182 - val_mae: 0.0922 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0151 - mae: 0.0736 - val_loss: 0.0147 - val_mae: 0.0784 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0643 - val_loss: 0.0133 - val_mae: 0.0726 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0674 - val_loss: 0.0129 - val_mae: 0.0724 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0131 - mae: 0.0646 - val_loss: 0.0124 - val_mae: 0.0700 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0630 - val_loss: 0.0119 - val_mae: 0.0674 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0630 - val_loss: 0.0118 - val_mae: 0.0687 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0608 - val_loss: 0.0116 - val_mae: 0.0666 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0124 - mae: 0.0626 - val_loss: 0.0114 - val_mae: 0.0683 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0649 - val_loss: 0.0116 - val_mae: 0.0682 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0634 - val_loss: 0.0112 - val_mae: 0.0670 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0121 - mae: 0.0615 - val_loss: 0.0124 - val_mae: 0.0708 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0120 - mae: 0.0605 - val_loss: 0.0110 - val_mae: 0.0661 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0121 - mae: 0.0615 - val_loss: 0.0129 - val_mae: 0.0731 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0120 - mae: 0.0615 - val_loss: 0.0113 - val_mae: 0.0676 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0117 - mae: 0.0606 - val_loss: 0.0112 - val_mae: 0.0663 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0119 - mae: 0.0621 - val_loss: 0.0113 - val_mae: 0.0709 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0632 - val_loss: 0.0135 - val_mae: 0.0761 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0118 - mae: 0.0610 - val_loss: 0.0112 - val_mae: 0.0664 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0116 - mae: 0.0605 - val_loss: 0.0111 - val_mae: 0.0663 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0116 - mae: 0.0605 - val_loss: 0.0111 - val_mae: 0.0657 - lr: 2.5000e-04\n",
      "  co_max1_value - Test MSE: 0.0171, Test MAE: 0.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: co_max1_hour\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 15.1773 - mae: 2.9675 - val_loss: 2.4810 - val_mae: 1.2032 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.9511 - mae: 1.3469 - val_loss: 2.1533 - val_mae: 1.1084 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.5516 - mae: 1.2558 - val_loss: 1.8747 - val_mae: 1.0351 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1771 - mae: 1.1565 - val_loss: 1.6118 - val_mae: 0.9426 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7810 - mae: 1.0482 - val_loss: 1.4482 - val_mae: 0.8914 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6364 - mae: 1.0033 - val_loss: 1.4109 - val_mae: 0.8691 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4381 - mae: 0.9338 - val_loss: 1.2695 - val_mae: 0.8287 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4561 - mae: 0.9429 - val_loss: 1.2756 - val_mae: 0.8245 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3824 - mae: 0.9119 - val_loss: 1.5424 - val_mae: 0.9354 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3283 - mae: 0.8902 - val_loss: 1.2396 - val_mae: 0.8156 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2740 - mae: 0.8825 - val_loss: 1.5037 - val_mae: 0.9211 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2401 - mae: 0.8694 - val_loss: 1.2861 - val_mae: 0.8303 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2619 - mae: 0.8577 - val_loss: 1.2693 - val_mae: 0.8209 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2398 - mae: 0.8642 - val_loss: 1.5975 - val_mae: 0.9545 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3121 - mae: 0.8898 - val_loss: 1.5743 - val_mae: 0.9539 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1155 - mae: 0.8160 - val_loss: 1.2674 - val_mae: 0.8219 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1127 - mae: 0.8143 - val_loss: 1.8754 - val_mae: 1.0774 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1096 - mae: 0.8200 - val_loss: 1.2906 - val_mae: 0.8323 - lr: 5.0000e-04\n",
      "  co_max1_hour - Test MSE: 18.1192, Test MAE: 3.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: co_aqi\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 4.1461 - mae: 1.5007 - val_loss: 2.3062 - val_mae: 1.1707 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4770 - mae: 1.0919 - val_loss: 1.6349 - val_mae: 0.9223 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6781 - mae: 0.8389 - val_loss: 1.4897 - val_mae: 0.8801 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4980 - mae: 0.7812 - val_loss: 1.1433 - val_mae: 0.7794 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3595 - mae: 0.7417 - val_loss: 1.0674 - val_mae: 0.7343 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2829 - mae: 0.7094 - val_loss: 1.5911 - val_mae: 0.9342 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2355 - mae: 0.6739 - val_loss: 1.0916 - val_mae: 0.7350 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1708 - mae: 0.6670 - val_loss: 1.0197 - val_mae: 0.7026 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1985 - mae: 0.6818 - val_loss: 1.0910 - val_mae: 0.7460 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0726 - mae: 0.6348 - val_loss: 1.0504 - val_mae: 0.7272 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1004 - mae: 0.6308 - val_loss: 1.1241 - val_mae: 0.7683 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9848 - mae: 0.5898 - val_loss: 1.0862 - val_mae: 0.7508 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0572 - mae: 0.6096 - val_loss: 1.0430 - val_mae: 0.7209 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9866 - mae: 0.5872 - val_loss: 1.0537 - val_mae: 0.7294 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9537 - mae: 0.5735 - val_loss: 0.9526 - val_mae: 0.6896 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9345 - mae: 0.5728 - val_loss: 0.8585 - val_mae: 0.6447 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9591 - mae: 0.5740 - val_loss: 0.8583 - val_mae: 0.6526 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9268 - mae: 0.5672 - val_loss: 1.0358 - val_mae: 0.7373 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9205 - mae: 0.5666 - val_loss: 0.9269 - val_mae: 0.6784 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9664 - mae: 0.5775 - val_loss: 0.9209 - val_mae: 0.6808 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9078 - mae: 0.5562 - val_loss: 0.7859 - val_mae: 0.6080 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9128 - mae: 0.5560 - val_loss: 1.1003 - val_mae: 0.7645 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.8955 - mae: 0.5723 - val_loss: 0.9678 - val_mae: 0.7001 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9426 - mae: 0.5727 - val_loss: 1.1042 - val_mae: 0.7678 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.8825 - mae: 0.5610 - val_loss: 1.2940 - val_mae: 0.8622 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9043 - mae: 0.5611 - val_loss: 1.1405 - val_mae: 0.7918 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.8990 - mae: 0.5453 - val_loss: 0.8648 - val_mae: 0.6517 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9106 - mae: 0.5587 - val_loss: 0.9842 - val_mae: 0.7110 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.8651 - mae: 0.5351 - val_loss: 0.9298 - val_mae: 0.6825 - lr: 2.5000e-04\n",
      "  co_aqi - Test MSE: 0.8638, Test MAE: 0.6542\n",
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: no2_mean\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 24.2720 - mae: 3.7941 - val_loss: 9.9860 - val_mae: 2.4252 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9240 - mae: 2.0562 - val_loss: 3.0524 - val_mae: 1.3144 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.4216 - mae: 1.3519 - val_loss: 1.8429 - val_mae: 1.0118 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.8781 - mae: 1.2259 - val_loss: 1.6054 - val_mae: 0.9489 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4442 - mae: 1.1512 - val_loss: 1.5168 - val_mae: 0.9188 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2117 - mae: 1.0751 - val_loss: 1.4283 - val_mae: 0.8963 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2617 - mae: 1.0906 - val_loss: 1.4131 - val_mae: 0.8790 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1847 - mae: 1.0738 - val_loss: 1.2586 - val_mae: 0.8374 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0561 - mae: 1.0254 - val_loss: 1.1916 - val_mae: 0.8099 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9505 - mae: 1.0100 - val_loss: 1.3894 - val_mae: 0.8872 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8908 - mae: 0.9768 - val_loss: 1.2733 - val_mae: 0.8511 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9142 - mae: 0.9944 - val_loss: 1.1634 - val_mae: 0.8070 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0457 - mae: 1.0333 - val_loss: 1.2812 - val_mae: 0.8365 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6883 - mae: 0.9527 - val_loss: 1.1243 - val_mae: 0.7943 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7976 - mae: 0.9715 - val_loss: 1.1594 - val_mae: 0.8139 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7149 - mae: 0.9295 - val_loss: 1.0524 - val_mae: 0.7460 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7011 - mae: 0.9241 - val_loss: 1.0484 - val_mae: 0.7428 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6305 - mae: 0.9189 - val_loss: 1.1613 - val_mae: 0.8003 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6590 - mae: 0.9334 - val_loss: 1.0083 - val_mae: 0.7253 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5270 - mae: 0.8890 - val_loss: 1.0776 - val_mae: 0.7615 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5912 - mae: 0.8950 - val_loss: 1.1184 - val_mae: 0.7762 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5184 - mae: 0.8812 - val_loss: 1.2346 - val_mae: 0.8227 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5760 - mae: 0.8968 - val_loss: 1.1982 - val_mae: 0.8003 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5272 - mae: 0.8843 - val_loss: 1.0637 - val_mae: 0.7545 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3820 - mae: 0.8332 - val_loss: 1.0601 - val_mae: 0.7558 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3562 - mae: 0.8281 - val_loss: 1.2241 - val_mae: 0.8204 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3071 - mae: 0.8099 - val_loss: 1.2814 - val_mae: 0.8455 - lr: 5.0000e-04\n",
      "  no2_mean - Test MSE: 2.2537, Test MAE: 1.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: no2_max1_value\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 162.1033 - mae: 10.7507 - val_loss: 46.1926 - val_mae: 5.3165 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 45.6565 - mae: 5.3806 - val_loss: 34.8984 - val_mae: 4.7050 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26.6053 - mae: 3.7991 - val_loss: 10.0860 - val_mae: 2.4160 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 12.2005 - mae: 2.5961 - val_loss: 7.5190 - val_mae: 2.0094 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.6651 - mae: 2.3072 - val_loss: 6.3740 - val_mae: 1.8473 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.2900 - mae: 2.3037 - val_loss: 6.5598 - val_mae: 1.8683 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.2879 - mae: 2.1418 - val_loss: 5.9879 - val_mae: 1.7743 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.2702 - mae: 2.1376 - val_loss: 6.2510 - val_mae: 1.8261 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8840 - mae: 2.1089 - val_loss: 5.0568 - val_mae: 1.6629 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3337 - mae: 2.0179 - val_loss: 5.3963 - val_mae: 1.6975 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.0449 - mae: 1.9432 - val_loss: 5.4076 - val_mae: 1.7055 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.0502 - mae: 1.9839 - val_loss: 5.3363 - val_mae: 1.6838 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.0524 - mae: 1.9841 - val_loss: 6.2388 - val_mae: 1.9273 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.9728 - mae: 1.9756 - val_loss: 5.5508 - val_mae: 1.6989 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.8617 - mae: 1.9198 - val_loss: 5.1303 - val_mae: 1.6506 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.5226 - mae: 1.8855 - val_loss: 5.0741 - val_mae: 1.6396 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.1444 - mae: 1.8393 - val_loss: 4.9512 - val_mae: 1.6143 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.9507 - mae: 1.8033 - val_loss: 4.9320 - val_mae: 1.6205 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.0507 - mae: 1.8148 - val_loss: 4.6801 - val_mae: 1.5990 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.5769 - mae: 1.8720 - val_loss: 5.2008 - val_mae: 1.6613 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.7033 - mae: 1.7770 - val_loss: 5.0402 - val_mae: 1.6328 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.8613 - mae: 1.7967 - val_loss: 5.7272 - val_mae: 1.7487 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.1839 - mae: 1.8186 - val_loss: 5.8163 - val_mae: 1.7449 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.5953 - mae: 1.7695 - val_loss: 5.7568 - val_mae: 1.7465 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.8100 - mae: 1.7657 - val_loss: 5.2312 - val_mae: 1.6632 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.3328 - mae: 1.7203 - val_loss: 5.3380 - val_mae: 1.6692 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.5716 - mae: 1.7456 - val_loss: 5.2171 - val_mae: 1.6416 - lr: 2.5000e-04\n",
      "  no2_max1_value - Test MSE: 8.8074, Test MAE: 2.1116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: no2_max1_hour\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 86.2619 - mae: 8.7683 - val_loss: 11.9001 - val_mae: 3.0553 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.8413 - mae: 1.7182 - val_loss: 4.6179 - val_mae: 1.3932 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0247 - mae: 1.5819 - val_loss: 3.4024 - val_mae: 1.2541 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.6173 - mae: 1.4496 - val_loss: 1.7462 - val_mae: 1.0012 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.9734 - mae: 1.3168 - val_loss: 2.0405 - val_mae: 1.1321 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.6483 - mae: 1.2487 - val_loss: 1.3571 - val_mae: 0.8670 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.3143 - mae: 1.1758 - val_loss: 1.2168 - val_mae: 0.8048 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2029 - mae: 1.1486 - val_loss: 1.4892 - val_mae: 0.9189 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0886 - mae: 1.1018 - val_loss: 1.2493 - val_mae: 0.8159 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2093 - mae: 1.1325 - val_loss: 1.3412 - val_mae: 0.8563 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0833 - mae: 1.1023 - val_loss: 1.4171 - val_mae: 0.8958 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9666 - mae: 1.0569 - val_loss: 1.2072 - val_mae: 0.8052 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7364 - mae: 1.0024 - val_loss: 2.0994 - val_mae: 1.1490 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9594 - mae: 1.0430 - val_loss: 1.8700 - val_mae: 1.0812 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7880 - mae: 1.0138 - val_loss: 1.4566 - val_mae: 0.9113 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7614 - mae: 1.0018 - val_loss: 1.6708 - val_mae: 0.9984 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6970 - mae: 1.0023 - val_loss: 1.3785 - val_mae: 0.8765 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6393 - mae: 0.9583 - val_loss: 1.7534 - val_mae: 1.0298 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5381 - mae: 0.9452 - val_loss: 1.2580 - val_mae: 0.8226 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5274 - mae: 0.9437 - val_loss: 1.2293 - val_mae: 0.8130 - lr: 5.0000e-04\n",
      "  no2_max1_hour - Test MSE: 1.2940, Test MAE: 0.7597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: no2_aqi\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 106.3963 - mae: 8.3060 - val_loss: 33.2443 - val_mae: 4.6570 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 27.6224 - mae: 3.9948 - val_loss: 8.4869 - val_mae: 2.1076 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.8660 - mae: 2.3249 - val_loss: 6.3626 - val_mae: 1.8795 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.4812 - mae: 2.1536 - val_loss: 5.5245 - val_mae: 1.7666 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8451 - mae: 2.0565 - val_loss: 5.2154 - val_mae: 1.7188 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4823 - mae: 2.0160 - val_loss: 4.9519 - val_mae: 1.6564 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.1920 - mae: 1.9765 - val_loss: 5.1398 - val_mae: 1.7105 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.0531 - mae: 1.9642 - val_loss: 4.5807 - val_mae: 1.6105 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.6630 - mae: 1.9099 - val_loss: 4.5093 - val_mae: 1.5838 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.1591 - mae: 1.8251 - val_loss: 4.7323 - val_mae: 1.6098 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.3065 - mae: 1.8873 - val_loss: 5.1119 - val_mae: 1.6698 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.2702 - mae: 1.8350 - val_loss: 4.4095 - val_mae: 1.5428 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.9846 - mae: 1.8324 - val_loss: 4.5640 - val_mae: 1.5820 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.2354 - mae: 1.8491 - val_loss: 4.4338 - val_mae: 1.5491 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.0050 - mae: 1.8308 - val_loss: 4.5443 - val_mae: 1.5870 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.6703 - mae: 1.7571 - val_loss: 4.6250 - val_mae: 1.5989 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.7211 - mae: 1.7605 - val_loss: 4.4277 - val_mae: 1.5277 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.5084 - mae: 1.7333 - val_loss: 4.4746 - val_mae: 1.5461 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.5040 - mae: 1.7267 - val_loss: 4.2913 - val_mae: 1.5305 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.3336 - mae: 1.6767 - val_loss: 4.7215 - val_mae: 1.5752 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.8614 - mae: 1.6273 - val_loss: 4.1508 - val_mae: 1.4920 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.2400 - mae: 1.6848 - val_loss: 4.3071 - val_mae: 1.5190 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0946 - mae: 1.6508 - val_loss: 4.5333 - val_mae: 1.5438 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.6699 - mae: 1.7326 - val_loss: 4.7567 - val_mae: 1.5921 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0334 - mae: 1.6572 - val_loss: 4.0698 - val_mae: 1.4815 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.1340 - mae: 1.6566 - val_loss: 4.3479 - val_mae: 1.5075 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.9990 - mae: 1.6435 - val_loss: 4.4551 - val_mae: 1.5462 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0896 - mae: 1.6335 - val_loss: 4.4409 - val_mae: 1.5226 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.1709 - mae: 1.6539 - val_loss: 4.2177 - val_mae: 1.4961 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.8529 - mae: 1.6474 - val_loss: 4.4267 - val_mae: 1.5385 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0165 - mae: 1.6394 - val_loss: 4.2236 - val_mae: 1.4901 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.8097 - mae: 1.6255 - val_loss: 4.1491 - val_mae: 1.4837 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.7117 - mae: 1.5895 - val_loss: 4.0583 - val_mae: 1.4996 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.7108 - mae: 1.6046 - val_loss: 4.3996 - val_mae: 1.5388 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.8004 - mae: 1.6082 - val_loss: 4.1352 - val_mae: 1.4783 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.8618 - mae: 1.6103 - val_loss: 4.0612 - val_mae: 1.4725 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.6076 - mae: 1.5748 - val_loss: 4.1727 - val_mae: 1.4802 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.7083 - mae: 1.5860 - val_loss: 4.4645 - val_mae: 1.5376 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.7389 - mae: 1.5951 - val_loss: 4.1567 - val_mae: 1.4765 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.6298 - mae: 1.5884 - val_loss: 4.1904 - val_mae: 1.4712 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.3704 - mae: 1.5371 - val_loss: 4.2351 - val_mae: 1.4955 - lr: 1.2500e-04\n",
      "  no2_aqi - Test MSE: 7.3033, Test MAE: 1.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm25_max1_value\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 58.1489 - mae: 5.6642 - val_loss: 20.9732 - val_mae: 2.9306 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 21.8691 - mae: 2.6801 - val_loss: 21.8553 - val_mae: 2.8669 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 21.6476 - mae: 2.6386 - val_loss: 22.5092 - val_mae: 2.8735 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 20.9898 - mae: 2.5344 - val_loss: 20.3694 - val_mae: 2.6219 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 18.7674 - mae: 2.2769 - val_loss: 21.6763 - val_mae: 2.7161 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17.7534 - mae: 2.1853 - val_loss: 19.4717 - val_mae: 2.6355 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17.4791 - mae: 2.1824 - val_loss: 19.4663 - val_mae: 2.5563 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 16.6638 - mae: 2.1182 - val_loss: 18.6761 - val_mae: 2.4386 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17.2280 - mae: 2.1597 - val_loss: 17.9303 - val_mae: 2.3264 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 16.1172 - mae: 2.0539 - val_loss: 18.1331 - val_mae: 2.4295 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.8163 - mae: 2.0390 - val_loss: 19.1710 - val_mae: 2.4476 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.7924 - mae: 2.0389 - val_loss: 19.8167 - val_mae: 2.4818 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.0810 - mae: 1.9638 - val_loss: 19.2716 - val_mae: 2.4410 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.0494 - mae: 2.0509 - val_loss: 19.8398 - val_mae: 2.4687 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 14.8307 - mae: 1.9981 - val_loss: 20.0021 - val_mae: 2.4952 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 14.5530 - mae: 1.9762 - val_loss: 18.6223 - val_mae: 2.3830 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 14.4020 - mae: 1.9368 - val_loss: 18.6244 - val_mae: 2.4398 - lr: 5.0000e-04\n",
      "  pm25_max1_value - Test MSE: 11.6469, Test MAE: 2.0375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm25_max1_hour\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 8.1079 - mae: 2.3272 - val_loss: 2.8852 - val_mae: 1.5027 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4523 - mae: 1.2502 - val_loss: 0.4245 - val_mae: 0.4853 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.5537 - mae: 0.5764 - val_loss: 0.3819 - val_mae: 0.4328 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4849 - mae: 0.5408 - val_loss: 0.3846 - val_mae: 0.4260 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4160 - mae: 0.4894 - val_loss: 0.4873 - val_mae: 0.5270 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4406 - mae: 0.5123 - val_loss: 0.4727 - val_mae: 0.5191 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3816 - mae: 0.4775 - val_loss: 0.3327 - val_mae: 0.4002 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3404 - mae: 0.4426 - val_loss: 0.5332 - val_mae: 0.5789 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3356 - mae: 0.4433 - val_loss: 0.4155 - val_mae: 0.4933 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3444 - mae: 0.4424 - val_loss: 0.3773 - val_mae: 0.4556 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3238 - mae: 0.4362 - val_loss: 0.3605 - val_mae: 0.4389 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3086 - mae: 0.4230 - val_loss: 0.2934 - val_mae: 0.3768 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2994 - mae: 0.4124 - val_loss: 0.4584 - val_mae: 0.5261 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2952 - mae: 0.4057 - val_loss: 0.3079 - val_mae: 0.4031 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2900 - mae: 0.4053 - val_loss: 0.3307 - val_mae: 0.4225 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2869 - mae: 0.4030 - val_loss: 0.7378 - val_mae: 0.7314 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2724 - mae: 0.3914 - val_loss: 0.7734 - val_mae: 0.7502 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2528 - mae: 0.3735 - val_loss: 0.7182 - val_mae: 0.7131 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2405 - mae: 0.3618 - val_loss: 0.8216 - val_mae: 0.7793 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2465 - mae: 0.3679 - val_loss: 0.7512 - val_mae: 0.7370 - lr: 5.0000e-04\n",
      "  pm25_max1_hour - Test MSE: 0.9118, Test MAE: 0.5600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm25_aqi\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 887.6661 - mae: 27.2612 - val_loss: 334.5424 - val_mae: 15.4014 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 131.8918 - mae: 8.7083 - val_loss: 116.0643 - val_mae: 8.1600 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 102.8476 - mae: 7.5619 - val_loss: 84.1447 - val_mae: 6.6850 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 86.1523 - mae: 6.5755 - val_loss: 88.1343 - val_mae: 6.5634 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 79.8194 - mae: 6.2800 - val_loss: 83.0436 - val_mae: 6.3133 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 72.0315 - mae: 5.8614 - val_loss: 73.8961 - val_mae: 6.0072 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 69.2265 - mae: 5.7299 - val_loss: 70.6662 - val_mae: 5.7754 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 67.8403 - mae: 5.7536 - val_loss: 67.4558 - val_mae: 5.6811 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 62.2579 - mae: 5.5087 - val_loss: 67.1825 - val_mae: 5.7162 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 60.1889 - mae: 5.3921 - val_loss: 65.3799 - val_mae: 5.6072 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 60.4879 - mae: 5.4144 - val_loss: 73.9595 - val_mae: 6.0223 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 59.1524 - mae: 5.2573 - val_loss: 61.1857 - val_mae: 5.4857 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 56.8417 - mae: 5.1810 - val_loss: 62.0135 - val_mae: 5.4282 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 57.3306 - mae: 5.2263 - val_loss: 59.7088 - val_mae: 5.3313 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 55.8139 - mae: 5.2582 - val_loss: 59.8773 - val_mae: 5.3901 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 54.1405 - mae: 5.1029 - val_loss: 59.4745 - val_mae: 5.5398 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 55.0814 - mae: 5.2715 - val_loss: 61.4855 - val_mae: 5.4133 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 56.2980 - mae: 5.1374 - val_loss: 63.7263 - val_mae: 5.5094 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 51.7584 - mae: 4.9651 - val_loss: 70.4794 - val_mae: 5.8509 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 51.1732 - mae: 5.0143 - val_loss: 57.8717 - val_mae: 5.3208 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 51.8441 - mae: 5.0759 - val_loss: 61.0308 - val_mae: 5.4767 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 50.2691 - mae: 4.8240 - val_loss: 68.3918 - val_mae: 5.7274 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 48.8606 - mae: 4.8578 - val_loss: 59.0102 - val_mae: 5.3985 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 50.1027 - mae: 4.8276 - val_loss: 59.9236 - val_mae: 5.3684 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 49.1472 - mae: 4.7837 - val_loss: 60.9729 - val_mae: 5.4346 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 44.9972 - mae: 4.6709 - val_loss: 61.8030 - val_mae: 5.4191 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 45.3135 - mae: 4.7220 - val_loss: 58.0505 - val_mae: 5.2804 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 44.5754 - mae: 4.5811 - val_loss: 55.0642 - val_mae: 5.2115 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 42.5414 - mae: 4.5563 - val_loss: 61.0645 - val_mae: 5.4153 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 47.1958 - mae: 4.7740 - val_loss: 62.8707 - val_mae: 5.4940 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 44.5311 - mae: 4.6522 - val_loss: 65.6248 - val_mae: 5.6548 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 45.1026 - mae: 4.6075 - val_loss: 63.3889 - val_mae: 5.5122 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 42.8347 - mae: 4.5345 - val_loss: 54.2738 - val_mae: 5.1928 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 43.0387 - mae: 4.5426 - val_loss: 73.2032 - val_mae: 5.9478 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 45.7097 - mae: 4.5516 - val_loss: 59.9859 - val_mae: 5.3916 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 42.9004 - mae: 4.5553 - val_loss: 63.6871 - val_mae: 5.5664 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 43.1135 - mae: 4.6334 - val_loss: 60.2929 - val_mae: 5.4319 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 42.5876 - mae: 4.5840 - val_loss: 57.4624 - val_mae: 5.2670 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 39.5646 - mae: 4.3868 - val_loss: 58.3769 - val_mae: 5.3366 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 40.3283 - mae: 4.4172 - val_loss: 64.3097 - val_mae: 5.5500 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 40.0727 - mae: 4.3666 - val_loss: 61.0396 - val_mae: 5.4223 - lr: 2.5000e-04\n",
      "  pm25_aqi - Test MSE: 36.7919, Test MAE: 4.5491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm10_mean\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 300.1564 - mae: 15.6201 - val_loss: 245.4192 - val_mae: 13.9464 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 118.0532 - mae: 7.9968 - val_loss: 52.6498 - val_mae: 5.2253 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 58.0726 - mae: 5.3175 - val_loss: 51.0685 - val_mae: 5.1550 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 46.7751 - mae: 4.4243 - val_loss: 34.6032 - val_mae: 3.7621 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 35.0317 - mae: 3.7452 - val_loss: 31.2149 - val_mae: 3.5041 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 33.3291 - mae: 3.6328 - val_loss: 30.2055 - val_mae: 3.3651 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 30.5989 - mae: 3.5215 - val_loss: 29.1012 - val_mae: 3.4164 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 29.8669 - mae: 3.4489 - val_loss: 33.2180 - val_mae: 3.9811 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 29.4379 - mae: 3.4324 - val_loss: 28.9976 - val_mae: 3.5195 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 27.0526 - mae: 3.2879 - val_loss: 28.8041 - val_mae: 3.2820 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26.5564 - mae: 3.2239 - val_loss: 30.4087 - val_mae: 3.5636 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 27.5104 - mae: 3.2896 - val_loss: 30.8225 - val_mae: 3.7773 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26.4688 - mae: 3.2443 - val_loss: 30.1636 - val_mae: 3.5869 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26.3011 - mae: 3.1840 - val_loss: 29.6232 - val_mae: 3.5000 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26.0893 - mae: 3.2186 - val_loss: 32.9682 - val_mae: 3.8429 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.3595 - mae: 3.1110 - val_loss: 30.6765 - val_mae: 3.6465 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 24.7654 - mae: 3.0897 - val_loss: 29.9690 - val_mae: 3.5750 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 24.6356 - mae: 3.0978 - val_loss: 29.8049 - val_mae: 3.5456 - lr: 5.0000e-04\n",
      "  pm10_mean - Test MSE: 38.7209, Test MAE: 3.4502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm10_max1_value\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 223.8753 - mae: 12.4950 - val_loss: 78.9032 - val_mae: 6.3589 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 59.6314 - mae: 5.3348 - val_loss: 53.7939 - val_mae: 5.2256 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 57.7118 - mae: 5.2496 - val_loss: 50.6393 - val_mae: 4.9967 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 45.2023 - mae: 4.3269 - val_loss: 30.9478 - val_mae: 3.7074 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 31.0302 - mae: 3.5321 - val_loss: 29.5170 - val_mae: 3.4043 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 29.4749 - mae: 3.4430 - val_loss: 31.7536 - val_mae: 3.8389 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 27.1377 - mae: 3.3068 - val_loss: 29.3232 - val_mae: 3.3406 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 29.2985 - mae: 3.4329 - val_loss: 28.3017 - val_mae: 3.3048 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.7918 - mae: 3.2097 - val_loss: 28.4526 - val_mae: 3.3962 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 27.6900 - mae: 3.2714 - val_loss: 28.8921 - val_mae: 3.4032 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 27.3294 - mae: 3.2088 - val_loss: 29.0688 - val_mae: 3.4375 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.8963 - mae: 3.1914 - val_loss: 29.8777 - val_mae: 3.5716 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 26.3920 - mae: 3.1972 - val_loss: 29.3602 - val_mae: 3.5196 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 24.8773 - mae: 3.1086 - val_loss: 29.2849 - val_mae: 3.5273 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 25.3647 - mae: 3.1483 - val_loss: 29.4521 - val_mae: 3.4857 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 24.5239 - mae: 3.0948 - val_loss: 29.4775 - val_mae: 3.4517 - lr: 5.0000e-04\n",
      "  pm10_max1_value - Test MSE: 34.5055, Test MAE: 3.3108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm10_max1_hour\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 187.3274 - mae: 11.5208 - val_loss: 44.9432 - val_mae: 5.9143 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.2301 - mae: 2.4547 - val_loss: 6.4422 - val_mae: 1.5091 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.3699 - mae: 1.5243 - val_loss: 6.1287 - val_mae: 1.2565 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.8296 - mae: 1.4305 - val_loss: 4.5366 - val_mae: 1.1274 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.5716 - mae: 1.3552 - val_loss: 4.0619 - val_mae: 0.9962 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.4657 - mae: 1.3239 - val_loss: 4.0460 - val_mae: 1.2445 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.5704 - mae: 1.3601 - val_loss: 4.2586 - val_mae: 1.0960 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.2043 - mae: 1.2683 - val_loss: 3.7647 - val_mae: 0.9956 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.1328 - mae: 1.2605 - val_loss: 3.1127 - val_mae: 0.8351 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.3939 - mae: 1.3146 - val_loss: 3.4710 - val_mae: 1.1472 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.2274 - mae: 1.2767 - val_loss: 3.5961 - val_mae: 0.9065 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.0838 - mae: 1.2453 - val_loss: 3.6593 - val_mae: 0.9768 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.8720 - mae: 1.2011 - val_loss: 2.9025 - val_mae: 0.8800 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.7080 - mae: 1.1430 - val_loss: 3.0217 - val_mae: 0.9338 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.7148 - mae: 1.1535 - val_loss: 3.2600 - val_mae: 0.9981 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.7646 - mae: 1.1631 - val_loss: 2.9601 - val_mae: 0.9810 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.5887 - mae: 1.1348 - val_loss: 3.0662 - val_mae: 0.9502 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.5954 - mae: 1.1433 - val_loss: 4.1393 - val_mae: 1.3364 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.5018 - mae: 1.1015 - val_loss: 2.9877 - val_mae: 0.9799 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4076 - mae: 1.0741 - val_loss: 3.2287 - val_mae: 1.0826 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.3185 - mae: 1.0751 - val_loss: 3.9110 - val_mae: 1.3078 - lr: 5.0000e-04\n",
      "  pm10_max1_hour - Test MSE: 5.4284, Test MAE: 1.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: pm10_aqi\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 144.9013 - mae: 9.8693 - val_loss: 41.4544 - val_mae: 4.6907 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 40.9725 - mae: 4.6543 - val_loss: 40.7049 - val_mae: 4.6582 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 38.7633 - mae: 4.4749 - val_loss: 37.3417 - val_mae: 4.4415 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 30.6263 - mae: 3.8328 - val_loss: 22.8795 - val_mae: 3.1599 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 22.0554 - mae: 3.2182 - val_loss: 20.7646 - val_mae: 2.8946 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 19.5752 - mae: 3.0349 - val_loss: 20.7466 - val_mae: 2.9759 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 18.3366 - mae: 2.9115 - val_loss: 21.1496 - val_mae: 3.0790 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17.9799 - mae: 2.9396 - val_loss: 20.2568 - val_mae: 2.9282 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17.7043 - mae: 2.8604 - val_loss: 20.6878 - val_mae: 2.9447 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17.9387 - mae: 2.8653 - val_loss: 23.8746 - val_mae: 3.4257 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 17.5174 - mae: 2.8583 - val_loss: 20.4889 - val_mae: 2.8873 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 16.8544 - mae: 2.7880 - val_loss: 20.4992 - val_mae: 2.8217 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 16.5851 - mae: 2.7868 - val_loss: 20.2600 - val_mae: 2.8493 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.3405 - mae: 2.6571 - val_loss: 20.4747 - val_mae: 2.9027 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.5648 - mae: 2.6759 - val_loss: 20.5074 - val_mae: 2.9149 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.9914 - mae: 2.7137 - val_loss: 20.4005 - val_mae: 2.8732 - lr: 5.0000e-04\n",
      "  pm10_aqi - Test MSE: 18.6274, Test MAE: 2.8495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: so2_mean\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.7752 - mae: 0.4466 - val_loss: 58.9370 - val_mae: 0.9667 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4205 - mae: 0.3367 - val_loss: 58.3857 - val_mae: 0.9853 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3420 - mae: 0.3139 - val_loss: 58.2206 - val_mae: 0.9284 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3048 - mae: 0.3045 - val_loss: 57.8331 - val_mae: 0.9044 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2576 - mae: 0.2836 - val_loss: 57.4595 - val_mae: 0.8768 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2036 - mae: 0.2643 - val_loss: 57.3028 - val_mae: 0.8706 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1773 - mae: 0.2557 - val_loss: 57.0986 - val_mae: 0.8713 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1833 - mae: 0.2470 - val_loss: 57.1933 - val_mae: 0.8786 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1791 - mae: 0.2501 - val_loss: 57.3340 - val_mae: 0.8460 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1524 - mae: 0.2339 - val_loss: 57.2441 - val_mae: 0.8655 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1601 - mae: 0.2356 - val_loss: 56.9806 - val_mae: 0.8770 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1664 - mae: 0.2376 - val_loss: 57.1242 - val_mae: 0.8666 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1414 - mae: 0.2293 - val_loss: 56.9550 - val_mae: 0.8313 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1654 - mae: 0.2303 - val_loss: 56.6354 - val_mae: 0.8533 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1527 - mae: 0.2325 - val_loss: 56.8522 - val_mae: 0.8678 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1660 - mae: 0.2294 - val_loss: 56.7602 - val_mae: 0.8379 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1338 - mae: 0.2190 - val_loss: 56.6405 - val_mae: 0.8349 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1628 - mae: 0.2198 - val_loss: 56.4330 - val_mae: 0.8188 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1411 - mae: 0.2148 - val_loss: 56.4678 - val_mae: 0.8420 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1377 - mae: 0.2147 - val_loss: 56.4529 - val_mae: 0.8458 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1315 - mae: 0.2043 - val_loss: 56.5290 - val_mae: 0.8359 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1148 - mae: 0.2032 - val_loss: 56.1849 - val_mae: 0.8332 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1274 - mae: 0.2192 - val_loss: 56.1032 - val_mae: 0.8241 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1588 - mae: 0.2256 - val_loss: 55.9654 - val_mae: 0.8934 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1310 - mae: 0.2122 - val_loss: 56.1659 - val_mae: 0.8234 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1202 - mae: 0.2057 - val_loss: 56.0252 - val_mae: 0.8318 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1359 - mae: 0.2093 - val_loss: 56.3183 - val_mae: 0.8207 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1125 - mae: 0.1963 - val_loss: 56.0968 - val_mae: 0.8175 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1128 - mae: 0.1942 - val_loss: 56.1241 - val_mae: 0.8189 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1156 - mae: 0.1902 - val_loss: 56.0148 - val_mae: 0.8155 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1186 - mae: 0.1925 - val_loss: 55.9160 - val_mae: 0.8248 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1044 - mae: 0.1902 - val_loss: 55.8656 - val_mae: 0.8126 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1074 - mae: 0.1911 - val_loss: 55.9178 - val_mae: 0.8103 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1090 - mae: 0.1947 - val_loss: 55.9847 - val_mae: 0.8127 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0898 - mae: 0.1791 - val_loss: 55.9331 - val_mae: 0.8134 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1022 - mae: 0.1878 - val_loss: 55.7091 - val_mae: 0.8071 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1043 - mae: 0.1878 - val_loss: 55.7741 - val_mae: 0.8110 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1051 - mae: 0.1908 - val_loss: 56.0521 - val_mae: 0.8047 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0976 - mae: 0.1809 - val_loss: 55.9549 - val_mae: 0.8123 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1075 - mae: 0.1857 - val_loss: 56.0387 - val_mae: 0.8115 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0955 - mae: 0.1844 - val_loss: 55.9578 - val_mae: 0.8161 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0911 - mae: 0.1776 - val_loss: 55.9369 - val_mae: 0.8113 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0906 - mae: 0.1769 - val_loss: 55.8974 - val_mae: 0.8100 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0889 - mae: 0.1761 - val_loss: 55.9255 - val_mae: 0.8051 - lr: 2.5000e-04\n",
      "  so2_mean - Test MSE: 0.0773, Test MAE: 0.1607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: so2_max1_value\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 4.2955 - mae: 1.2489 - val_loss: 132.9521 - val_mae: 1.9424 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2162 - mae: 0.8693 - val_loss: 131.1843 - val_mae: 1.5507 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3957 - mae: 0.7021 - val_loss: 130.2803 - val_mae: 1.5387 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3305 - mae: 0.6713 - val_loss: 129.7043 - val_mae: 1.5379 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1506 - mae: 0.6492 - val_loss: 130.3236 - val_mae: 1.4786 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9623 - mae: 0.5833 - val_loss: 129.9568 - val_mae: 1.4215 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.8804 - mae: 0.5778 - val_loss: 129.3908 - val_mae: 1.4558 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.8956 - mae: 0.6178 - val_loss: 128.9802 - val_mae: 1.4613 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7776 - mae: 0.5498 - val_loss: 128.9382 - val_mae: 1.4587 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7678 - mae: 0.5389 - val_loss: 129.2397 - val_mae: 1.4186 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7226 - mae: 0.5299 - val_loss: 129.6305 - val_mae: 1.3947 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7386 - mae: 0.5272 - val_loss: 129.2491 - val_mae: 1.3982 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7453 - mae: 0.5178 - val_loss: 130.1491 - val_mae: 1.4256 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6942 - mae: 0.5087 - val_loss: 129.2314 - val_mae: 1.3744 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.7152 - mae: 0.5066 - val_loss: 128.8290 - val_mae: 1.3838 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6707 - mae: 0.4873 - val_loss: 129.6818 - val_mae: 1.3729 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6529 - mae: 0.4964 - val_loss: 129.1571 - val_mae: 1.3867 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6648 - mae: 0.4902 - val_loss: 129.5723 - val_mae: 1.3682 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6146 - mae: 0.4707 - val_loss: 129.5005 - val_mae: 1.3722 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6531 - mae: 0.4851 - val_loss: 129.7817 - val_mae: 1.3849 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6818 - mae: 0.4799 - val_loss: 129.2036 - val_mae: 1.3810 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6390 - mae: 0.4700 - val_loss: 129.0798 - val_mae: 1.3826 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6203 - mae: 0.4690 - val_loss: 129.3521 - val_mae: 1.3823 - lr: 2.5000e-04\n",
      "  so2_max1_value - Test MSE: 0.5212, Test MAE: 0.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: so2_max1_hour\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 23.7598 - mae: 3.8770 - val_loss: 3.1569 - val_mae: 1.3100 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.4059 - mae: 1.4646 - val_loss: 3.0905 - val_mae: 1.3004 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.0743 - mae: 1.4073 - val_loss: 2.9730 - val_mae: 1.2752 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.9493 - mae: 1.3707 - val_loss: 2.7612 - val_mae: 1.2831 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4530 - mae: 1.2513 - val_loss: 1.6571 - val_mae: 0.9542 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9912 - mae: 1.1041 - val_loss: 1.5816 - val_mae: 0.9708 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6790 - mae: 1.0263 - val_loss: 1.2990 - val_mae: 0.8497 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6431 - mae: 0.9999 - val_loss: 1.2904 - val_mae: 0.8539 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4492 - mae: 0.9384 - val_loss: 1.1508 - val_mae: 0.8022 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4690 - mae: 0.9578 - val_loss: 1.3311 - val_mae: 0.8916 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3693 - mae: 0.9088 - val_loss: 1.5170 - val_mae: 0.9753 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4181 - mae: 0.9350 - val_loss: 1.1276 - val_mae: 0.8111 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3471 - mae: 0.9029 - val_loss: 1.1628 - val_mae: 0.8333 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3286 - mae: 0.8904 - val_loss: 1.2789 - val_mae: 0.8773 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1997 - mae: 0.8474 - val_loss: 1.4837 - val_mae: 0.9716 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2057 - mae: 0.8485 - val_loss: 1.3584 - val_mae: 0.9157 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2065 - mae: 0.8485 - val_loss: 1.0373 - val_mae: 0.7706 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0787 - mae: 0.8189 - val_loss: 1.0537 - val_mae: 0.7835 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1904 - mae: 0.8431 - val_loss: 1.2052 - val_mae: 0.8514 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0752 - mae: 0.8037 - val_loss: 1.4173 - val_mae: 0.9451 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0896 - mae: 0.8131 - val_loss: 1.3516 - val_mae: 0.9110 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0625 - mae: 0.7953 - val_loss: 1.8933 - val_mae: 1.1344 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0339 - mae: 0.7852 - val_loss: 1.4475 - val_mae: 0.9518 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0026 - mae: 0.7792 - val_loss: 1.7302 - val_mae: 1.0771 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9386 - mae: 0.7521 - val_loss: 1.6137 - val_mae: 1.0290 - lr: 5.0000e-04\n",
      "  so2_max1_hour - Test MSE: 1.3856, Test MAE: 0.8139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: so2_aqi\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 8.3327 - mae: 1.7850 - val_loss: 14.9354 - val_mae: 1.3433 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.3758 - mae: 1.0345 - val_loss: 14.3228 - val_mae: 1.0815 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.6604 - mae: 0.9408 - val_loss: 14.1804 - val_mae: 0.9788 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.3909 - mae: 0.9066 - val_loss: 13.6956 - val_mae: 1.0632 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0886 - mae: 0.8746 - val_loss: 13.9114 - val_mae: 0.9477 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6764 - mae: 0.8317 - val_loss: 13.5911 - val_mae: 0.9631 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5498 - mae: 0.8052 - val_loss: 13.2886 - val_mae: 1.0268 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4828 - mae: 0.7660 - val_loss: 13.3716 - val_mae: 0.9324 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4794 - mae: 0.7666 - val_loss: 13.3865 - val_mae: 0.9284 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2852 - mae: 0.7356 - val_loss: 13.6152 - val_mae: 0.8999 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3657 - mae: 0.7678 - val_loss: 13.2970 - val_mae: 0.9528 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1724 - mae: 0.7306 - val_loss: 13.3211 - val_mae: 0.9300 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2771 - mae: 0.7174 - val_loss: 13.2883 - val_mae: 0.8721 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1679 - mae: 0.7039 - val_loss: 13.1834 - val_mae: 0.8945 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1510 - mae: 0.6891 - val_loss: 13.2364 - val_mae: 0.8871 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1770 - mae: 0.6980 - val_loss: 13.2416 - val_mae: 0.8611 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1407 - mae: 0.6891 - val_loss: 13.2248 - val_mae: 0.8668 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1431 - mae: 0.6828 - val_loss: 13.3415 - val_mae: 0.8812 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1296 - mae: 0.6797 - val_loss: 13.2000 - val_mae: 0.9210 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1128 - mae: 0.6720 - val_loss: 13.3036 - val_mae: 0.8749 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.0018 - mae: 0.6559 - val_loss: 13.3082 - val_mae: 0.8644 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.1468 - mae: 0.6804 - val_loss: 13.3096 - val_mae: 0.8669 - lr: 2.5000e-04\n",
      "  so2_aqi - Test MSE: 1.1341, Test MAE: 0.6141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: o3_max1_hour\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 36.3111 - mae: 4.7810 - val_loss: 2.8529 - val_mae: 0.8725 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.1274 - mae: 1.2895 - val_loss: 2.5743 - val_mae: 0.6923 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.5311 - mae: 1.2011 - val_loss: 2.6251 - val_mae: 0.7754 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.3957 - mae: 1.1757 - val_loss: 2.3116 - val_mae: 0.7085 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.1690 - mae: 1.1473 - val_loss: 2.4009 - val_mae: 0.7961 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3.0065 - mae: 1.1008 - val_loss: 2.3960 - val_mae: 0.8350 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.9231 - mae: 1.1107 - val_loss: 2.2690 - val_mae: 0.7931 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.9207 - mae: 1.1113 - val_loss: 2.6615 - val_mae: 0.9893 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.7025 - mae: 1.0640 - val_loss: 1.9636 - val_mae: 0.7875 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.4844 - mae: 1.0292 - val_loss: 2.0959 - val_mae: 0.9110 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.3807 - mae: 1.0005 - val_loss: 2.4000 - val_mae: 0.9998 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2978 - mae: 0.9730 - val_loss: 2.0614 - val_mae: 0.9519 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1799 - mae: 0.9567 - val_loss: 2.0909 - val_mae: 0.9005 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2908 - mae: 0.9838 - val_loss: 2.1620 - val_mae: 0.9613 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.2178 - mae: 0.9556 - val_loss: 1.8412 - val_mae: 0.8814 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.1614 - mae: 0.9466 - val_loss: 2.1206 - val_mae: 1.0209 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0793 - mae: 0.9326 - val_loss: 2.3927 - val_mae: 1.0986 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2.0195 - mae: 0.9228 - val_loss: 1.8716 - val_mae: 0.9014 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9182 - mae: 0.9122 - val_loss: 2.4716 - val_mae: 1.1523 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.9215 - mae: 0.8859 - val_loss: 1.7904 - val_mae: 0.9041 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8249 - mae: 0.8693 - val_loss: 1.5759 - val_mae: 0.8299 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.8023 - mae: 0.8606 - val_loss: 2.2123 - val_mae: 1.1356 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.7569 - mae: 0.8765 - val_loss: 1.6779 - val_mae: 0.9392 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6409 - mae: 0.8320 - val_loss: 2.0612 - val_mae: 1.0134 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5437 - mae: 0.8081 - val_loss: 1.5915 - val_mae: 0.9322 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.5768 - mae: 0.8071 - val_loss: 1.3631 - val_mae: 0.9035 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.6156 - mae: 0.8444 - val_loss: 1.9846 - val_mae: 1.1467 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.4177 - mae: 0.7722 - val_loss: 1.7369 - val_mae: 1.0706 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3186 - mae: 0.7576 - val_loss: 1.6539 - val_mae: 0.9805 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3569 - mae: 0.7533 - val_loss: 1.7291 - val_mae: 1.0582 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2695 - mae: 0.7441 - val_loss: 2.1084 - val_mae: 1.2408 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2189 - mae: 0.7146 - val_loss: 1.8268 - val_mae: 1.1731 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3503 - mae: 0.7413 - val_loss: 2.5351 - val_mae: 1.3970 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.2940 - mae: 0.7132 - val_loss: 1.9687 - val_mae: 1.2090 - lr: 2.5000e-04\n",
      "  o3_max1_hour - Test MSE: 2.6646, Test MAE: 1.1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: awnd\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 4.0648 - mae: 1.5972 - val_loss: 1.0930 - val_mae: 0.8112 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 1.3023 - mae: 0.8925 - val_loss: 0.9581 - val_mae: 0.7546 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.9942 - mae: 0.7728 - val_loss: 0.6749 - val_mae: 0.6206 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.6415 - mae: 0.6080 - val_loss: 0.4251 - val_mae: 0.4862 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4788 - mae: 0.5167 - val_loss: 0.3703 - val_mae: 0.4496 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4487 - mae: 0.4913 - val_loss: 0.3181 - val_mae: 0.4029 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4168 - mae: 0.4724 - val_loss: 0.3120 - val_mae: 0.4012 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.4004 - mae: 0.4585 - val_loss: 0.3189 - val_mae: 0.4070 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3767 - mae: 0.4428 - val_loss: 0.3372 - val_mae: 0.4227 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3573 - mae: 0.4260 - val_loss: 0.2850 - val_mae: 0.3748 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3489 - mae: 0.4192 - val_loss: 0.3367 - val_mae: 0.4156 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3451 - mae: 0.4137 - val_loss: 0.2775 - val_mae: 0.3687 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3282 - mae: 0.3988 - val_loss: 0.2905 - val_mae: 0.3755 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3460 - mae: 0.4239 - val_loss: 0.2887 - val_mae: 0.3723 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3266 - mae: 0.4008 - val_loss: 0.2949 - val_mae: 0.3868 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3236 - mae: 0.3982 - val_loss: 0.3404 - val_mae: 0.4150 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3209 - mae: 0.3998 - val_loss: 0.2841 - val_mae: 0.3824 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3098 - mae: 0.3902 - val_loss: 0.3073 - val_mae: 0.3919 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2967 - mae: 0.3806 - val_loss: 0.2795 - val_mae: 0.3655 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3052 - mae: 0.3810 - val_loss: 0.2688 - val_mae: 0.3648 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.3008 - mae: 0.3805 - val_loss: 0.2771 - val_mae: 0.3654 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2917 - mae: 0.3742 - val_loss: 0.2901 - val_mae: 0.3850 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2910 - mae: 0.3751 - val_loss: 0.2626 - val_mae: 0.3619 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2889 - mae: 0.3747 - val_loss: 0.2625 - val_mae: 0.3539 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2885 - mae: 0.3778 - val_loss: 0.2744 - val_mae: 0.3678 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2754 - mae: 0.3614 - val_loss: 0.2664 - val_mae: 0.3594 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2756 - mae: 0.3594 - val_loss: 0.2715 - val_mae: 0.3599 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2779 - mae: 0.3623 - val_loss: 0.2870 - val_mae: 0.3771 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2831 - mae: 0.3631 - val_loss: 0.2922 - val_mae: 0.3761 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2730 - mae: 0.3608 - val_loss: 0.2636 - val_mae: 0.3533 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2605 - mae: 0.3466 - val_loss: 0.2961 - val_mae: 0.3875 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.2721 - mae: 0.3529 - val_loss: 0.3454 - val_mae: 0.4331 - lr: 2.5000e-04\n",
      "  awnd - Test MSE: 0.3282, Test MAE: 0.4193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: prcp\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 10571.4668 - mae: 74.7859 - val_loss: 7679.6968 - val_mae: 58.1237 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6841.4219 - mae: 57.1971 - val_loss: 4650.1777 - val_mae: 43.6820 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4153.1318 - mae: 42.5260 - val_loss: 3760.3567 - val_mae: 36.6227 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3611.9561 - mae: 40.6928 - val_loss: 3624.4976 - val_mae: 37.3531 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3531.1345 - mae: 40.1075 - val_loss: 3576.9634 - val_mae: 35.9282 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3418.7070 - mae: 39.4944 - val_loss: 3503.7871 - val_mae: 36.2952 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3391.5972 - mae: 39.8059 - val_loss: 3534.2209 - val_mae: 37.3103 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3381.5598 - mae: 39.3960 - val_loss: 3530.0200 - val_mae: 35.7299 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3322.4932 - mae: 39.3747 - val_loss: 3555.1162 - val_mae: 35.7956 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3216.4478 - mae: 38.3385 - val_loss: 3493.6626 - val_mae: 36.8459 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3243.5193 - mae: 38.6871 - val_loss: 3433.1199 - val_mae: 36.2955 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3205.7603 - mae: 38.6511 - val_loss: 3464.4221 - val_mae: 37.2754 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3130.0273 - mae: 38.8716 - val_loss: 3419.0215 - val_mae: 36.7769 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3118.1567 - mae: 38.5300 - val_loss: 3505.9800 - val_mae: 38.9290 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3122.1853 - mae: 38.4441 - val_loss: 3429.6628 - val_mae: 37.8494 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3029.8225 - mae: 38.2969 - val_loss: 3479.5066 - val_mae: 36.2857 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3036.0583 - mae: 38.4888 - val_loss: 3502.5620 - val_mae: 37.9700 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2997.8428 - mae: 37.9272 - val_loss: 3442.5703 - val_mae: 37.4028 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2925.8401 - mae: 37.3060 - val_loss: 3522.5762 - val_mae: 38.8556 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2917.7312 - mae: 37.2546 - val_loss: 3496.0942 - val_mae: 38.2354 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2894.3667 - mae: 37.1300 - val_loss: 3500.0537 - val_mae: 38.3039 - lr: 5.0000e-04\n",
      "  prcp - Test MSE: 2439.8723, Test MAE: 35.2474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: tmax\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 353.3812 - mae: 16.2792 - val_loss: 201.8602 - val_mae: 12.0096 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 108.4567 - mae: 8.5974 - val_loss: 28.3877 - val_mae: 4.1709 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 23.9984 - mae: 3.8477 - val_loss: 11.0499 - val_mae: 2.5428 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 16.1579 - mae: 3.0731 - val_loss: 8.8363 - val_mae: 2.2309 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 14.0603 - mae: 2.9030 - val_loss: 7.8130 - val_mae: 2.0706 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 13.3830 - mae: 2.8707 - val_loss: 8.0885 - val_mae: 2.1357 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 12.9287 - mae: 2.8065 - val_loss: 6.8915 - val_mae: 1.9341 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.6038 - mae: 2.6575 - val_loss: 7.4622 - val_mae: 2.0389 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.4871 - mae: 2.6318 - val_loss: 7.8255 - val_mae: 2.1221 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.0083 - mae: 2.5819 - val_loss: 8.4758 - val_mae: 2.1872 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.4588 - mae: 2.5138 - val_loss: 7.1258 - val_mae: 1.9738 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.7571 - mae: 2.5475 - val_loss: 6.7514 - val_mae: 1.9130 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.6955 - mae: 2.5447 - val_loss: 6.6201 - val_mae: 1.8679 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 10.4009 - mae: 2.5360 - val_loss: 6.6588 - val_mae: 1.9189 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.8470 - mae: 2.4380 - val_loss: 6.7985 - val_mae: 1.9285 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.5017 - mae: 2.3854 - val_loss: 6.2413 - val_mae: 1.8507 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.6118 - mae: 2.3670 - val_loss: 6.8361 - val_mae: 1.9274 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.7335 - mae: 2.4083 - val_loss: 6.4562 - val_mae: 1.8658 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.2249 - mae: 2.3687 - val_loss: 6.2046 - val_mae: 1.8189 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.1967 - mae: 2.3488 - val_loss: 6.5485 - val_mae: 1.8651 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.9620 - mae: 2.3176 - val_loss: 6.7598 - val_mae: 1.9497 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.1915 - mae: 2.3478 - val_loss: 6.4380 - val_mae: 1.8781 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.5173 - mae: 2.3984 - val_loss: 7.0622 - val_mae: 1.9634 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 9.5758 - mae: 2.4103 - val_loss: 6.4091 - val_mae: 1.8379 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.6773 - mae: 2.2724 - val_loss: 6.2807 - val_mae: 1.8330 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.4153 - mae: 2.2340 - val_loss: 6.1538 - val_mae: 1.7925 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.0676 - mae: 2.1884 - val_loss: 6.1172 - val_mae: 1.7899 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.8768 - mae: 2.1756 - val_loss: 6.3626 - val_mae: 1.8202 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.6302 - mae: 2.3061 - val_loss: 6.3084 - val_mae: 1.8271 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.0186 - mae: 2.2070 - val_loss: 6.2962 - val_mae: 1.8215 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.4000 - mae: 2.2596 - val_loss: 6.3200 - val_mae: 1.8147 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.7174 - mae: 2.2679 - val_loss: 6.0814 - val_mae: 1.7820 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.2304 - mae: 2.2109 - val_loss: 6.4798 - val_mae: 1.8652 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.2196 - mae: 2.2303 - val_loss: 6.0899 - val_mae: 1.7804 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.3904 - mae: 2.2265 - val_loss: 6.2279 - val_mae: 1.8212 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.1668 - mae: 2.2034 - val_loss: 6.2751 - val_mae: 1.8096 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.9765 - mae: 2.2039 - val_loss: 6.3060 - val_mae: 1.8332 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3956 - mae: 2.1279 - val_loss: 6.0951 - val_mae: 1.7868 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3343 - mae: 2.1240 - val_loss: 6.1267 - val_mae: 1.7834 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.4731 - mae: 2.1384 - val_loss: 6.1456 - val_mae: 1.7950 - lr: 2.5000e-04\n",
      "  tmax - Test MSE: 5.6078, Test MAE: 1.7347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Stage 2: Multivariate AR LSTM for target: tmin\n",
      "======================================================================\n",
      "  Shapes: train(1800, 12, 9), val(600, 12, 9), test(350, 12, 9)\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 130.1020 - mae: 9.3855 - val_loss: 81.1513 - val_mae: 7.4956 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 45.2230 - mae: 5.0965 - val_loss: 13.8529 - val_mae: 2.5272 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 15.2943 - mae: 2.7974 - val_loss: 7.8902 - val_mae: 2.0715 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 11.3723 - mae: 2.5644 - val_loss: 5.5885 - val_mae: 1.7706 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 8.4037 - mae: 2.2163 - val_loss: 6.4098 - val_mae: 1.9478 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.7839 - mae: 2.1443 - val_loss: 4.6882 - val_mae: 1.6532 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.6914 - mae: 2.1123 - val_loss: 4.5817 - val_mae: 1.6046 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 7.3157 - mae: 2.0496 - val_loss: 3.9865 - val_mae: 1.5098 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.7911 - mae: 1.9922 - val_loss: 5.3536 - val_mae: 1.8520 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.3125 - mae: 1.9426 - val_loss: 3.6919 - val_mae: 1.4570 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.4475 - mae: 1.9405 - val_loss: 4.3764 - val_mae: 1.6311 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.3892 - mae: 1.9666 - val_loss: 4.0882 - val_mae: 1.5522 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 6.3871 - mae: 1.9434 - val_loss: 3.9446 - val_mae: 1.5598 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.7059 - mae: 1.8368 - val_loss: 3.9472 - val_mae: 1.5517 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.5056 - mae: 1.8191 - val_loss: 4.2537 - val_mae: 1.5781 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.6808 - mae: 1.8414 - val_loss: 3.5265 - val_mae: 1.4349 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.4331 - mae: 1.7846 - val_loss: 3.5422 - val_mae: 1.4530 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.1298 - mae: 1.7478 - val_loss: 3.7941 - val_mae: 1.5080 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.2301 - mae: 1.7316 - val_loss: 3.9510 - val_mae: 1.5668 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.1013 - mae: 1.7513 - val_loss: 3.7494 - val_mae: 1.4628 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.2795 - mae: 1.7915 - val_loss: 3.9704 - val_mae: 1.5048 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.0324 - mae: 1.7337 - val_loss: 4.2075 - val_mae: 1.5869 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4.9202 - mae: 1.7040 - val_loss: 3.6248 - val_mae: 1.4661 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 5.1158 - mae: 1.7263 - val_loss: 3.6276 - val_mae: 1.4643 - lr: 2.5000e-04\n",
      "  tmin - Test MSE: 5.4713, Test MAE: 1.7010\n",
      "\n",
      "Saved forecasts for ALL targets (with top exogenous features) to: all_features_future_AR_with_top_exog_2025-08_to_2030-12.csv\n",
      "     state  year    month  pm25_mean_pred     vmt_pred  flights_pred  \\\n",
      "0  Alabama  2025  2025-08        8.909400  4911.245117  25708.355469   \n",
      "1  Alabama  2025  2025-09        9.204036  4892.287598  25708.355469   \n",
      "2  Alabama  2025  2025-10        8.782427  4924.093750  25708.355469   \n",
      "3  Alabama  2025  2025-11        8.382743  4959.955566  25708.355469   \n",
      "4  Alabama  2025  2025-12        8.275459  4774.865723  25708.355469   \n",
      "\n",
      "   co_mean_pred  co_max1_value_pred  co_max1_hour_pred  co_aqi_pred  ...  \\\n",
      "0      0.209862            0.326655           5.592048     2.480445  ...   \n",
      "1      0.223011            0.346793           6.296309     3.257927  ...   \n",
      "2      0.233888            0.393333           7.271080     3.802643  ...   \n",
      "3      0.250042            0.443748           7.798821     4.167298  ...   \n",
      "4      0.251938            0.469841           8.108092     4.257761  ...   \n",
      "\n",
      "   o3_max1_hour_pred  awnd_pred   prcp_pred  tmax_pred  tmin_pred  ndvi_pred  \\\n",
      "0           9.516860   2.177210  108.940529  32.308376  21.902210   0.759487   \n",
      "1           9.528392   2.177517  113.208435  30.308767  20.265177   0.743202   \n",
      "2           9.564965   2.171990   95.746208  24.991550  14.224057   0.703784   \n",
      "3           9.818361   2.305000   97.261040  19.555136   7.834962   0.639441   \n",
      "4          10.189876   2.423514   92.136940  15.497985   3.445443   0.590099   \n",
      "\n",
      "   o3_max1_value_pred  o3_mean_pred  o3_aqi_pred  tavg_pred  \n",
      "0            0.039845      0.030358    44.075966  26.816086  \n",
      "1            0.037912      0.030351    39.611179  24.494019  \n",
      "2            0.035955      0.030340    34.558659  19.129278  \n",
      "3            0.034234      0.030325    29.721718  13.366772  \n",
      "4            0.032875      0.030308    27.957464   9.593839  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# AR LSTM FOR ALL FEATURES WITH TOP EXOGENOUS DRIVERS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "df_ar = full.copy()\n",
    "\n",
    "# Ensure month is Timestamp\n",
    "if isinstance(df_ar[\"month\"].dtype, pd.PeriodDtype):\n",
    "    df_ar[\"month\"] = df_ar[\"month\"].dt.to_timestamp()\n",
    "else:\n",
    "    df_ar[\"month\"] = pd.to_datetime(df_ar[\"month\"])\n",
    "\n",
    "df_ar = df_ar.sort_values([\"state\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "# Time features\n",
    "df_ar[\"month_num\"] = df_ar[\"month\"].dt.month\n",
    "df_ar[\"month_sin\"] = np.sin(2 * np.pi * df_ar[\"month_num\"] / 12)\n",
    "df_ar[\"month_cos\"] = np.cos(2 * np.pi * df_ar[\"month_num\"] / 12)\n",
    "\n",
    "# Targets to forecast\n",
    "ar_targets = [\"pm25_mean\"] + all_target_cols\n",
    "ar_targets = list(dict.fromkeys(ar_targets))  # remove duplicates if any\n",
    "\n",
    "print(\"All targets to forecast:\", ar_targets)\n",
    "\n",
    "# Top global drivers from above LSTM\n",
    "top_exog = [\"month_num\", \"ndvi\", \"o3_max1_value\", \"o3_mean\", \"o3_aqi\", \"tavg\"]\n",
    "\n",
    "# Exogenous subset that actually needs forecasting (exclude purely time month_num)\n",
    "top_exog_to_forecast = [\"ndvi\", \"o3_max1_value\", \"o3_mean\", \"o3_aqi\", \"tavg\"]\n",
    "\n",
    "print(\"Top exogenous features (to be forecasted first):\", top_exog_to_forecast)\n",
    "\n",
    "train_end = pd.to_datetime(\"2021-12-01\")\n",
    "val_end   = pd.to_datetime(\"2023-12-01\")\n",
    "\n",
    "train_df = df_ar[df_ar[\"month\"] <= train_end].copy()\n",
    "val_df   = df_ar[(df_ar[\"month\"] > train_end) & (df_ar[\"month\"] <= val_end)].copy()\n",
    "test_df  = df_ar[df_ar[\"month\"] > val_end].copy()\n",
    "\n",
    "print(\"Train period:\", train_df[\"month\"].min(), \"->\", train_df[\"month\"].max())\n",
    "print(\"Val period:  \", val_df[\"month\"].min(),   \"->\", val_df[\"month\"].max())\n",
    "print(\"Test period: \", test_df[\"month\"].min(),  \"->\", test_df[\"month\"].max())\n",
    "\n",
    "seq_len = 12  # past 12 months\n",
    "\n",
    "def build_univariate_sequences(df, target_col, seq_len):\n",
    "    \"\"\"\n",
    "    For univariate AR + time\n",
    "    \"\"\"\n",
    "    feature_cols = [target_col, \"month_sin\", \"month_cos\", \"month_num\"]\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for state, g in df.groupby(\"state\"):\n",
    "        g = g.sort_values(\"month\")\n",
    "        if len(g) <= seq_len:\n",
    "            continue\n",
    "\n",
    "        feat = g[feature_cols].values\n",
    "        targ = g[target_col].values\n",
    "\n",
    "        for i in range(len(g) - seq_len):\n",
    "            X_list.append(feat[i:i+seq_len])\n",
    "            y_list.append(targ[i+seq_len])\n",
    "\n",
    "    X = np.array(X_list, dtype=\"float32\")\n",
    "    y = np.array(y_list, dtype=\"float32\")\n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "def build_multivar_sequences_for_target(df, target_col, seq_len, top_exog):\n",
    "    \"\"\"\n",
    "    For target + time + top exogenous features.\n",
    "    \"\"\"\n",
    "    base_feats = [\"month_sin\", \"month_cos\", \"month_num\"] + top_exog\n",
    "    feature_cols = [target_col] + [f for f in base_feats if f != target_col]\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for state, g in df.groupby(\"state\"):\n",
    "        g = g.sort_values(\"month\")\n",
    "        if len(g) <= seq_len:\n",
    "            continue\n",
    "\n",
    "        feat = g[feature_cols].values\n",
    "        targ = g[target_col].values\n",
    "\n",
    "        for i in range(len(g) - seq_len):\n",
    "            X_list.append(feat[i:i+seq_len])\n",
    "            y_list.append(targ[i+seq_len])\n",
    "\n",
    "    X = np.array(X_list, dtype=\"float32\")\n",
    "    y = np.array(y_list, dtype=\"float32\")\n",
    "    return X, y, feature_cols\n",
    "\n",
    "def build_lstm_model(n_timesteps, n_features, l2_lambda=1e-4):\n",
    "    l2_reg = regularizers.l2(l2_lambda)\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(n_timesteps, n_features)),\n",
    "        layers.LSTM(64, return_sequences=True,\n",
    "                    kernel_regularizer=l2_reg,\n",
    "                    recurrent_regularizer=l2_reg),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32, return_sequences=False,\n",
    "                    kernel_regularizer=l2_reg,\n",
    "                    recurrent_regularizer=l2_reg),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation=\"relu\", kernel_regularizer=l2_reg),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def seed_windows(df_hist, feature_cols, seq_len):\n",
    "    windows = {}\n",
    "    for st, g in df_hist.sort_values([\"state\",\"month\"]).groupby(\"state\"):\n",
    "        g_tail = g[feature_cols].tail(seq_len)\n",
    "        Xw = g_tail.values.astype(\"float32\")\n",
    "        if len(g_tail) < seq_len:\n",
    "            pad = np.repeat(Xw[:1], seq_len - len(g_tail), axis=0)\n",
    "            Xw = np.vstack([pad, Xw])\n",
    "        windows[st] = Xw\n",
    "    return windows\n",
    "\n",
    "def recursive_univariate_forecast(model, df_hist, future_calendar,\n",
    "                                  target_col, seq_len, feature_cols):\n",
    "\n",
    "    df_hist = df_hist.sort_values([\"state\",\"month\"]).copy()\n",
    "    windows = seed_windows(df_hist, feature_cols, seq_len)\n",
    "    preds = []\n",
    "\n",
    "    col_to_idx = {c: i for i, c in enumerate(feature_cols)}\n",
    "\n",
    "    for st, g_fut in future_calendar.sort_values([\"state\",\"month\"]).groupby(\"state\"):\n",
    "        if st not in windows:\n",
    "            continue\n",
    "\n",
    "        w = windows[st].copy().astype(\"float32\")\n",
    "\n",
    "        for _, row in g_fut.iterrows():\n",
    "            x_input = w.reshape(1, seq_len, len(feature_cols)).astype(\"float32\")\n",
    "            y_hat = float(model.predict(x_input, verbose=0).ravel()[0])\n",
    "\n",
    "            preds.append((st, row[\"month\"], y_hat))\n",
    "\n",
    "            new_feat = w[-1].copy()\n",
    "            # update target\n",
    "            new_feat[col_to_idx[target_col]] = y_hat\n",
    "            # update time features\n",
    "            new_feat[col_to_idx[\"month_num\"]] = row[\"month_num\"]\n",
    "            new_feat[col_to_idx[\"month_sin\"]] = row[\"month_sin\"]\n",
    "            new_feat[col_to_idx[\"month_cos\"]] = row[\"month_cos\"]\n",
    "\n",
    "            w = np.vstack([w[1:], new_feat])\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"state\", \"month\", f\"{target_col}_pred\"]\n",
    "    ).sort_values([\"state\", \"month\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def recursive_multivar_forecast_for_target(model, df_hist, future_merge,\n",
    "                                           target_col, seq_len, feature_cols, top_exog):\n",
    "    \"\"\"\n",
    "      For future steps:\n",
    "        - time features from future_merge\n",
    "        - top_exog from their _pred columns in future_merge\n",
    "        - target updated with model prediction\n",
    "    \"\"\"\n",
    "    df_hist = df_hist.sort_values([\"state\",\"month\"]).copy()\n",
    "    windows = seed_windows(df_hist, feature_cols, seq_len)\n",
    "    preds = []\n",
    "\n",
    "    col_to_idx = {c: i for i, c in enumerate(feature_cols)}\n",
    "\n",
    "    for st, g_fut in future_merge.sort_values([\"state\", \"month\"]).groupby(\"state\"):\n",
    "        if st not in windows:\n",
    "            continue\n",
    "\n",
    "        w = windows[st].copy().astype(\"float32\")\n",
    "\n",
    "        for _, row in g_fut.iterrows():\n",
    "            x_input = w.reshape(1, seq_len, len(feature_cols)).astype(\"float32\")\n",
    "            y_hat = float(model.predict(x_input, verbose=0).ravel()[0])\n",
    "\n",
    "            preds.append((st, row[\"month\"], y_hat))\n",
    "\n",
    "            new_feat = w[-1].copy()\n",
    "\n",
    "            # Update target\n",
    "            new_feat[col_to_idx[target_col]] = y_hat\n",
    "\n",
    "            # Update time features\n",
    "            if \"month_num\" in col_to_idx:\n",
    "                new_feat[col_to_idx[\"month_num\"]] = row[\"month_num\"]\n",
    "            if \"month_sin\" in col_to_idx:\n",
    "                new_feat[col_to_idx[\"month_sin\"]] = row[\"month_sin\"]\n",
    "            if \"month_cos\" in col_to_idx:\n",
    "                new_feat[col_to_idx[\"month_cos\"]] = row[\"month_cos\"]\n",
    "\n",
    "            # Update exogenous features from their predicted paths\n",
    "            for ex in top_exog:\n",
    "                if ex in col_to_idx:\n",
    "                    col_pred = ex + \"_pred\"\n",
    "                    if col_pred in row.index:\n",
    "                        new_feat[col_to_idx[ex]] = row[col_pred]\n",
    "\n",
    "            w = np.vstack([w[1:], new_feat])\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        preds,\n",
    "        columns=[\"state\", \"month\", f\"{target_col}_pred\"]\n",
    "    ).sort_values([\"state\",\"month\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "hist_cut     = pd.to_datetime(\"2025-07-01\")   # last observed month\n",
    "future_start = pd.to_datetime(\"2025-08-01\")\n",
    "future_end   = pd.to_datetime(\"2030-12-01\")\n",
    "\n",
    "future_idx = pd.MultiIndex.from_product(\n",
    "    [df_ar[\"state\"].unique(), pd.date_range(future_start, future_end, freq=\"MS\")],\n",
    "    names=[\"state\",\"month\"]\n",
    ").to_frame(index=False)\n",
    "\n",
    "future_idx[\"month_num\"] = future_idx[\"month\"].dt.month\n",
    "future_idx[\"month_sin\"] = np.sin(2 * np.pi * future_idx[\"month_num\"] / 12)\n",
    "future_idx[\"month_cos\"] = np.cos(2 * np.pi * future_idx[\"month_num\"] / 12)\n",
    "\n",
    "exog_future = future_idx.copy()  # will accumulate top_exog predictions\n",
    "\n",
    "for target_col in top_exog_to_forecast:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Stage 1: Univariate AR LSTM for exogenous target: {target_col}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if target_col not in df_ar.columns:\n",
    "        print(f\"  [SKIP] {target_col} not found in df_ar.columns\")\n",
    "        continue\n",
    "\n",
    "    train_X, train_y, feat_cols_uni = build_univariate_sequences(train_df, target_col, seq_len)\n",
    "    val_X,   val_y,   _            = build_univariate_sequences(val_df,   target_col, seq_len)\n",
    "    test_X,  test_y,  _            = build_univariate_sequences(test_df,  target_col, seq_len)\n",
    "\n",
    "    if len(train_X) == 0 or len(val_X) == 0 or len(test_X) == 0:\n",
    "        print(f\"  [SKIP] Not enough sequences for {target_col}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  Shapes: train{train_X.shape}, val{val_X.shape}, test{test_X.shape}\")\n",
    "\n",
    "    model = build_lstm_model(train_X.shape[1], train_X.shape[2], l2_lambda=1e-4)\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=8, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_X, train_y,\n",
    "        validation_data=(val_X, val_y),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    test_loss, test_mae = model.evaluate(test_X, test_y, verbose=0)\n",
    "    print(f\"  {target_col} - Test MSE: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    df_hist_t = df_ar[df_ar[\"month\"] <= hist_cut].copy()\n",
    "\n",
    "    pred_df = recursive_univariate_forecast(\n",
    "        model=model,\n",
    "        df_hist=df_hist_t,\n",
    "        future_calendar=future_idx,\n",
    "        target_col=target_col,\n",
    "        seq_len=seq_len,\n",
    "        feature_cols=feat_cols_uni\n",
    "    )\n",
    "\n",
    "    # Merge predicted exogenous path\n",
    "    exog_future = exog_future.merge(pred_df, on=[\"state\",\"month\"], how=\"left\")\n",
    "\n",
    "# Forecast remaining targets with top_exog as inputs\n",
    "\n",
    "future_merge = exog_future.copy()\n",
    "\n",
    "future_all = future_idx[[\"state\",\"month\"]].copy()\n",
    "\n",
    "non_exog_targets = [t for t in ar_targets if t not in top_exog_to_forecast]\n",
    "\n",
    "for target_col in non_exog_targets:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Stage 2: Multivariate AR LSTM for target: {target_col}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if target_col not in df_ar.columns:\n",
    "        print(f\"  [SKIP] {target_col} not found in df_ar.columns\")\n",
    "        continue\n",
    "\n",
    "    train_X, train_y, feat_cols = build_multivar_sequences_for_target(\n",
    "        train_df, target_col, seq_len, top_exog=top_exog_to_forecast\n",
    "    )\n",
    "    val_X,   val_y,   _        = build_multivar_sequences_for_target(\n",
    "        val_df,   target_col, seq_len, top_exog=top_exog_to_forecast\n",
    "    )\n",
    "    test_X,  test_y,  _        = build_multivar_sequences_for_target(\n",
    "        test_df,  target_col, seq_len, top_exog=top_exog_to_forecast\n",
    "    )\n",
    "\n",
    "    if len(train_X) == 0 or len(val_X) == 0 or len(test_X) == 0:\n",
    "        print(f\"  [SKIP] Not enough sequences for {target_col}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"  Shapes: train{train_X.shape}, val{val_X.shape}, test{test_X.shape}\")\n",
    "\n",
    "    model = build_lstm_model(train_X.shape[1], train_X.shape[2], l2_lambda=1e-4)\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=8, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_X, train_y,\n",
    "        validation_data=(val_X, val_y),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    test_loss, test_mae = model.evaluate(test_X, test_y, verbose=0)\n",
    "    print(f\"  {target_col} - Test MSE: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    # Historical data up to hist_cut for seeding\n",
    "    df_hist_t = df_ar[df_ar[\"month\"] <= hist_cut].copy()\n",
    "\n",
    "    pred_df = recursive_multivar_forecast_for_target(\n",
    "        model=model,\n",
    "        df_hist=df_hist_t,\n",
    "        future_merge=future_merge,\n",
    "        target_col=target_col,\n",
    "        seq_len=seq_len,\n",
    "        feature_cols=feat_cols,\n",
    "        top_exog=top_exog_to_forecast\n",
    "    )\n",
    "\n",
    "    future_all = future_all.merge(pred_df, on=[\"state\",\"month\"], how=\"left\")\n",
    "\n",
    "exog_pred_cols = [f\"{c}_pred\" for c in top_exog_to_forecast]\n",
    "future_all = future_all.merge(\n",
    "    exog_future[[\"state\",\"month\"] + exog_pred_cols],\n",
    "    on=[\"state\",\"month\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "future_all[\"year\"]      = future_all[\"month\"].dt.year\n",
    "future_all[\"month_num\"] = future_all[\"month\"].dt.month\n",
    "future_all[\"month\"]     = future_all[\"month\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "pred_cols = [c for c in future_all.columns if c.endswith(\"_pred\")]\n",
    "cols_order = [\"state\", \"year\", \"month\"] + pred_cols\n",
    "future_all = future_all[cols_order]\n",
    "\n",
    "output_path = \"all_features_future_AR_with_top_exog_2025-08_to_2030-12.csv\"\n",
    "future_all.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved forecasts for ALL targets (with top exogenous features) to: {output_path}\")\n",
    "print(future_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a41693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MasterDataset_with_future_AR.csv\n",
      "     state    month     vmt      ndvi  flights   co_mean  co_max1_value  \\\n",
      "0  Alabama  2018-01  5101.0  0.552518   6819.0  0.270312       0.527258   \n",
      "1  Alabama  2018-02  4975.0  0.541536   6417.0  0.219343       0.402143   \n",
      "2  Alabama  2018-03  5952.0  0.565882   7507.0  0.226716       0.407704   \n",
      "3  Alabama  2018-04  6145.0  0.679999   7541.0  0.214604       0.392311   \n",
      "4  Alabama  2018-05  6253.0  0.761965   8002.0  0.302609       0.490366   \n",
      "\n",
      "   co_max1_hour    co_aqi   no2_mean  ...  o3_max1_hour     o3_aqi      awnd  \\\n",
      "0     10.153226  5.209677  12.320599  ...     11.854839  30.725806  3.119355   \n",
      "1     10.383929  3.946429   8.593592  ...     21.219246  22.709325  3.514286   \n",
      "2      8.952151  3.747312   9.844573  ...     10.468993  40.165997  3.558065   \n",
      "3      8.936111  3.705556   8.937356  ...     10.388948  45.017963  3.526667   \n",
      "4      9.212366  4.946237  11.115237  ...      9.495067  45.172110  2.141935   \n",
      "\n",
      "    prcp       tavg       tmax       tmin  year  month_num  observed  \n",
      "0   34.4   4.341935  10.332258  -1.167742  2018        1.0       1.0  \n",
      "1  206.8  14.110714  19.650000   9.039286  2018        2.0       1.0  \n",
      "2  104.9  12.977419  19.358065   6.551613  2018        3.0       1.0  \n",
      "3  209.6  15.720000  22.576667   8.696667  2018        4.0       1.0  \n",
      "4  177.2  23.754839  29.883871  18.393548  2018        5.0       1.0  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "old_path = \"MasterDataset_interpolated_final.csv\"     \n",
    "new_path = \"all_features_future_AR_with_top_exog_2025-08_to_2030-12.csv\"  \n",
    "\n",
    "old_df = pd.read_csv(old_path)\n",
    "new_df = pd.read_csv(new_path)\n",
    "\n",
    "old_df[\"month\"] = pd.to_datetime(old_df[\"month\"])\n",
    "new_df[\"month\"] = pd.to_datetime(new_df[\"month\"])\n",
    "\n",
    "pred_cols = [c for c in new_df.columns if c.endswith(\"_pred\")]\n",
    "rename_map = {c: c.replace(\"_pred\", \"\") for c in pred_cols}\n",
    "new_df = new_df.rename(columns=rename_map)\n",
    "\n",
    "for col in old_df.columns:\n",
    "    if col not in new_df.columns:\n",
    "        new_df[col] = np.nan\n",
    "\n",
    "new_df = new_df[old_df.columns]\n",
    "\n",
    "combined = pd.concat([old_df, new_df], ignore_index=True)\n",
    "\n",
    "combined = combined.drop_duplicates(subset=[\"state\", \"month\"], keep=\"first\")\n",
    "\n",
    "combined = combined.sort_values([\"state\", \"month\"]).reset_index(drop=True)\n",
    "combined[\"month\"] = combined[\"month\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "combined.to_csv(\"MasterDataset_AR.csv\", index=False)\n",
    "print(\"Saved MasterDataset_with_future_AR.csv\")\n",
    "print(combined.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('MasterDataset_AR.csv')  \n",
    "df2 = pd.read_csv('MasterDataset_with_health_outcome.csv')  \n",
    "\n",
    "merged_df = pd.merge(df1, df2[['state', 'month', 'ihd_deaths', 'copd_deaths', 'asthma_deaths']], \n",
    "                     on=['state', 'month'], \n",
    "                     how='left')\n",
    "\n",
    "merged_df.to_csv('MasterDataset_AR_health.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dva_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
